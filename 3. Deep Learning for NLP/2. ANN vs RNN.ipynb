{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afa90d8",
   "metadata": {},
   "source": [
    "## ü§ñ ANN vs RNN ‚Äì Understanding the Difference in NLP\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Artificial Neural Networks (ANN)\n",
    "\n",
    "An **Artificial Neural Network (ANN)** is a basic feedforward network where data flows in one direction: from input ‚Üí hidden layers ‚Üí output.\n",
    "\n",
    "In NLP tasks like **sentiment analysis**, ANN can still be used if we first **convert text to fixed-length vectors** using techniques such as:\n",
    "\n",
    "- Bag of Words (BoW)\n",
    "- TF-IDF\n",
    "- Word2Vec (static embeddings)\n",
    "\n",
    "Once the text is vectorized, we feed the entire sentence as a **single input vector** into the ANN. It treats this vector like any other data point ‚Äî **without understanding the sequence of words**.\n",
    "\n",
    "#### ‚ö†Ô∏è Limitation:\n",
    "- **No understanding of word order**\n",
    "- **Context and sequence are lost**\n",
    "- Treats all inputs **independently**\n",
    "- Good for text classification, but not ideal for tasks like translation or text generation\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Example: How ANN Handles Text\n",
    "\n",
    "**Sentence**: `\"The movie was not good\"`  \n",
    "After TF-IDF or BOW, it's converted into a vector like:  \n",
    "`[0, 1, 1, 0, 1, 0, 0, 1]`  \n",
    "\n",
    "The order of words is **completely lost**.  \n",
    "So `\"not good\"` and `\"good not\"` might look the same ‚Äî leading to **loss of meaning**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Recurrent Neural Networks (RNN)\n",
    "\n",
    "An **RNN** is designed specifically to work with **sequential data**, where the **order of inputs matters** ‚Äî like text, audio, or time series.\n",
    "\n",
    "#### üß† Key Concept:\n",
    "RNNs process data **one element at a time**, maintaining a **hidden state** that stores information about previous inputs.  \n",
    "This creates a **\"memory\"** of what the model has seen so far.\n",
    "\n",
    "---\n",
    "\n",
    "### üìç How RNN Handles Sequences\n",
    "\n",
    "Instead of feeding the whole sentence at once, like in ANN, RNNs feed **one word at a time**:\n",
    "\n",
    "- **Time step 1**: Input = \"The\" ‚Üí hidden state‚ÇÅ\n",
    "- **Time step 2**: Input = \"movie\" + hidden state‚ÇÅ ‚Üí hidden state‚ÇÇ\n",
    "- **Time step 3**: Input = \"was\" + hidden state‚ÇÇ ‚Üí hidden state‚ÇÉ\n",
    "- ...\n",
    "\n",
    "Each time step updates the hidden state and **passes it forward**, creating a **feedback loop** ‚Äî a defining feature of RNNs.\n",
    "\n",
    "This allows the model to **remember past information** and use it to influence future predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ The Feedback Loop in RNNs\n",
    "\n",
    "- At every time step, the RNN takes **two inputs**:\n",
    "  - The **current word**\n",
    "  - The **hidden state** (memory) from the previous step\n",
    "- It combines them and updates the hidden state\n",
    "- The hidden state is then passed to the **next step in the sequence**\n",
    "- This is how the network **retains and shares context across time**\n",
    "\n",
    "---\n",
    "\n",
    "### üåê Where Do We Use RNNs?\n",
    "\n",
    "Tasks that require understanding **word order** and **context**:\n",
    "- Text generation\n",
    "- Google Translate (machine translation)\n",
    "- Speech recognition\n",
    "- Named Entity Recognition (NER)\n",
    "- Language modeling\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Simple Comparison:\n",
    "\n",
    "| Feature                | ANN (Feedforward)                       | RNN (Recurrent)                             |\n",
    "|------------------------|-----------------------------------------|---------------------------------------------|\n",
    "| Input Handling         | Whole sentence at once (as a vector)    | One word at a time (step-by-step)           |\n",
    "| Sequence Awareness     | ‚ùå No                                   | ‚úÖ Yes                                       |\n",
    "| Memory/Context         | ‚ùå None                                 | ‚úÖ Maintains hidden state                    |\n",
    "| Good For               | Classification, regression              | Text generation, translation, sequential tasks |\n",
    "| Feedback Mechanism     | ‚ùå No feedback                          | ‚úÖ Feedback loop: passes info to next step   |\n",
    "\n",
    "---\n",
    "\n",
    "üí° In summary:  \n",
    "- **ANN** is great when word order doesn‚Äôt matter (like classification tasks with BoW or TF-IDF).  \n",
    "- But when **sequence and context matter** ‚Äî like in translation or text generation ‚Äî we need **RNNs** that process text **one step at a time**, with memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738999e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
