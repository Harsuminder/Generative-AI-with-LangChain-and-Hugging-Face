{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4003dc7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” GRU vs LSTM RNN: Simplifying Memory Management\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Why LSTM RNNs Can Be Complex\n",
    "\n",
    "LSTM RNNs were designed to solve the **long-term dependency** issue found in vanilla RNNs by introducing a **memory cell (`Câ‚œ`)** that acts like long-term memory and a hidden state (`hâ‚œ`) that captures short-term memory.\n",
    "\n",
    "LSTMs use **three gates**:\n",
    "\n",
    "- **Forget Gate (`fâ‚œ`)** â€“ decides what to forget from the past memory.\n",
    "- **Input Gate (`iâ‚œ`)** â€“ decides what new information to store.\n",
    "- **Output Gate (`oâ‚œ`)** â€“ decides what to output at this time step.\n",
    "\n",
    "Each gate comes with its **own set of weights (`Wf`, `Wi`, `Wo`, `Wc`) and biases**, all of which are **trainable parameters**.\n",
    "\n",
    "As a result:\n",
    "- The model becomes **parameter-heavy**.\n",
    "- **Training time increases**.\n",
    "- Model tuning and regularization become more difficult.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Gated Recurrent Unit (GRU): A Simpler Alternative\n",
    "\n",
    "To address the **complexity of LSTM RNNs**, GRUs were introduced.\n",
    "\n",
    "GRUs **combine the memory roles** and remove the cell state `Câ‚œ` altogether. Instead, they maintain a **single hidden state `hâ‚œ`** to capture both long-term and short-term dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Key Features of GRU\n",
    "\n",
    "- **Fewer gates** â†’ Only 2 gates:\n",
    "  1. **Update Gate (`zâ‚œ`)**: Controls how much of the past information to retain.\n",
    "  2. **Reset Gate (`râ‚œ`)**: Controls how much of the past to forget during candidate activation.\n",
    "\n",
    "- **No separate memory cell** â†’ The hidden state `hâ‚œ` serves both as short-term and long-term memory.\n",
    "- **Fewer parameters** â†’ Faster training and reduced risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§® GRU Equations\n",
    "\n",
    "Let `xâ‚œ` be the input and `hâ‚œâ‚‹â‚` be the previous hidden state.\n",
    "\n",
    "- **Update Gate**:  \n",
    "  `zâ‚œ = Ïƒ(Wz Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bz)`\n",
    "\n",
    "- **Reset Gate**:  \n",
    "  `râ‚œ = Ïƒ(Wr Â· [hâ‚œâ‚‹â‚, xâ‚œ] + br)`\n",
    "\n",
    "- **Candidate Hidden State**:  \n",
    "  `hÌƒâ‚œ = tanh(W Â· [râ‚œ * hâ‚œâ‚‹â‚, xâ‚œ] + b)`\n",
    "\n",
    "- **Final Hidden State (Output)**:  \n",
    "  `hâ‚œ = (1 - zâ‚œ) * hâ‚œâ‚‹â‚ + zâ‚œ * hÌƒâ‚œ`\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Interpretation of GRU Flow\n",
    "\n",
    "- `zâ‚œ` close to 1 â†’ **Use new information** from `hÌƒâ‚œ`.\n",
    "- `zâ‚œ` close to 0 â†’ **Keep previous memory** `hâ‚œâ‚‹â‚`.\n",
    "- `râ‚œ` controls how much of the past should influence the candidate.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary\n",
    "\n",
    "| Feature               | LSTM RNN                      | GRU                        |\n",
    "|-----------------------|-------------------------------|----------------------------|\n",
    "| Gates                 | 3 (forget, input, output)      | 2 (update, reset)          |\n",
    "| Separate Memory Cell  | Yes (`Câ‚œ`)                    | No                         |\n",
    "| Hidden State          | `hâ‚œ`                          | `hâ‚œ`                       |\n",
    "| Parameters            | More                          | Fewer                      |\n",
    "| Training Time         | Longer                        | Faster                     |\n",
    "| Performance           | Strong long-term memory       | Competitive on many tasks  |\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¡ **GRUs** offer a **simplified yet powerful alternative** to LSTM RNNs, especially when:\n",
    "- You want faster training,\n",
    "- The dataset is small or medium-sized,\n",
    "- You donâ€™t need the fine-grained control of LSTMâ€™s gates.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923be9e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ Coupled Memory Update in GRU: Understanding Final Hidden State `hâ‚œ`\n",
    "\n",
    "In a GRU, the final hidden state `hâ‚œ` is computed as a **weighted combination** of the previous hidden state and the new candidate hidden state:\n",
    "\n",
    "$$\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  How the Update Gate `zâ‚œ` Works\n",
    "\n",
    "- `zâ‚œ` is the **update gate**, a value between 0 and 1 (after applying the sigmoid function).\n",
    "- It decides **how much of the new candidate** (`ğ‘¯Ìƒâ‚œ`) should be used, and **how much of the past** (`hâ‚œâ‚‹â‚`) should be retained.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”— Coupled Behavior\n",
    "\n",
    "This equation is **coupled** because:\n",
    "\n",
    "- If `zâ‚œ` is **high** (close to 1):\n",
    "  - More weight is given to `ğ‘¯Ìƒâ‚œ` â†’ the GRU emphasizes the **current input**.\n",
    "  - `(1 - zâ‚œ)` becomes small â†’ **less of `hâ‚œâ‚‹â‚` is retained**.\n",
    "  - The model **updates** itself with new information.\n",
    "\n",
    "- If `zâ‚œ` is **low** (close to 0):\n",
    "  - More weight is given to `hâ‚œâ‚‹â‚` â†’ GRU retains **previous context**.\n",
    "  - `zâ‚œ` becomes small â†’ **little new information is added**.\n",
    "  - The model **remembers** old information.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Interpretation\n",
    "\n",
    "- GRU acts like a **blend of memory and update**.\n",
    "- Instead of separately controlling what to forget and what to add (as in **LSTM RNNs**), GRU **couples** these two actions.\n",
    "- This makes the architecture **simpler**, more **computationally efficient**, and with **fewer parameters** to learn.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff974aaa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
