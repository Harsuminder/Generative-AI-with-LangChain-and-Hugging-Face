{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4003dc7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔁 GRU vs LSTM RNN: Simplifying Memory Management\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why LSTM RNNs Can Be Complex\n",
    "\n",
    "LSTM RNNs were designed to solve the **long-term dependency** issue found in vanilla RNNs by introducing a **memory cell (`Cₜ`)** that acts like long-term memory and a hidden state (`hₜ`) that captures short-term memory.\n",
    "\n",
    "LSTMs use **three gates**:\n",
    "\n",
    "- **Forget Gate (`fₜ`)** – decides what to forget from the past memory.\n",
    "- **Input Gate (`iₜ`)** – decides what new information to store.\n",
    "- **Output Gate (`oₜ`)** – decides what to output at this time step.\n",
    "\n",
    "Each gate comes with its **own set of weights (`Wf`, `Wi`, `Wo`, `Wc`) and biases**, all of which are **trainable parameters**.\n",
    "\n",
    "As a result:\n",
    "- The model becomes **parameter-heavy**.\n",
    "- **Training time increases**.\n",
    "- Model tuning and regularization become more difficult.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Gated Recurrent Unit (GRU): A Simpler Alternative\n",
    "\n",
    "To address the **complexity of LSTM RNNs**, GRUs were introduced.\n",
    "\n",
    "GRUs **combine the memory roles** and remove the cell state `Cₜ` altogether. Instead, they maintain a **single hidden state `hₜ`** to capture both long-term and short-term dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Key Features of GRU\n",
    "\n",
    "- **Fewer gates** → Only 2 gates:\n",
    "  1. **Update Gate (`zₜ`)**: Controls how much of the past information to retain.\n",
    "  2. **Reset Gate (`rₜ`)**: Controls how much of the past to forget during candidate activation.\n",
    "\n",
    "- **No separate memory cell** → The hidden state `hₜ` serves both as short-term and long-term memory.\n",
    "- **Fewer parameters** → Faster training and reduced risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 GRU Equations\n",
    "\n",
    "Let `xₜ` be the input and `hₜ₋₁` be the previous hidden state.\n",
    "\n",
    "- **Update Gate**:  \n",
    "  `zₜ = σ(Wz · [hₜ₋₁, xₜ] + bz)`\n",
    "\n",
    "- **Reset Gate**:  \n",
    "  `rₜ = σ(Wr · [hₜ₋₁, xₜ] + br)`\n",
    "\n",
    "- **Candidate Hidden State**:  \n",
    "  `h̃ₜ = tanh(W · [rₜ * hₜ₋₁, xₜ] + b)`\n",
    "\n",
    "- **Final Hidden State (Output)**:  \n",
    "  `hₜ = (1 - zₜ) * hₜ₋₁ + zₜ * h̃ₜ`\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Interpretation of GRU Flow\n",
    "\n",
    "- `zₜ` close to 1 → **Use new information** from `h̃ₜ`.\n",
    "- `zₜ` close to 0 → **Keep previous memory** `hₜ₋₁`.\n",
    "- `rₜ` controls how much of the past should influence the candidate.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "\n",
    "| Feature               | LSTM RNN                      | GRU                        |\n",
    "|-----------------------|-------------------------------|----------------------------|\n",
    "| Gates                 | 3 (forget, input, output)      | 2 (update, reset)          |\n",
    "| Separate Memory Cell  | Yes (`Cₜ`)                    | No                         |\n",
    "| Hidden State          | `hₜ`                          | `hₜ`                       |\n",
    "| Parameters            | More                          | Fewer                      |\n",
    "| Training Time         | Longer                        | Faster                     |\n",
    "| Performance           | Strong long-term memory       | Competitive on many tasks  |\n",
    "\n",
    "---\n",
    "\n",
    "💡 **GRUs** offer a **simplified yet powerful alternative** to LSTM RNNs, especially when:\n",
    "- You want faster training,\n",
    "- The dataset is small or medium-sized,\n",
    "- You don’t need the fine-grained control of LSTM’s gates.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923be9e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 Coupled Memory Update in GRU: Understanding Final Hidden State `hₜ`\n",
    "\n",
    "In a GRU, the final hidden state `hₜ` is computed as a **weighted combination** of the previous hidden state and the new candidate hidden state:\n",
    "\n",
    "$$\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 How the Update Gate `zₜ` Works\n",
    "\n",
    "- `zₜ` is the **update gate**, a value between 0 and 1 (after applying the sigmoid function).\n",
    "- It decides **how much of the new candidate** (`𝑯̃ₜ`) should be used, and **how much of the past** (`hₜ₋₁`) should be retained.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 Coupled Behavior\n",
    "\n",
    "This equation is **coupled** because:\n",
    "\n",
    "- If `zₜ` is **high** (close to 1):\n",
    "  - More weight is given to `𝑯̃ₜ` → the GRU emphasizes the **current input**.\n",
    "  - `(1 - zₜ)` becomes small → **less of `hₜ₋₁` is retained**.\n",
    "  - The model **updates** itself with new information.\n",
    "\n",
    "- If `zₜ` is **low** (close to 0):\n",
    "  - More weight is given to `hₜ₋₁` → GRU retains **previous context**.\n",
    "  - `zₜ` becomes small → **little new information is added**.\n",
    "  - The model **remembers** old information.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Interpretation\n",
    "\n",
    "- GRU acts like a **blend of memory and update**.\n",
    "- Instead of separately controlling what to forget and what to add (as in **LSTM RNNs**), GRU **couples** these two actions.\n",
    "- This makes the architecture **simpler**, more **computationally efficient**, and with **fewer parameters** to learn.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff974aaa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
