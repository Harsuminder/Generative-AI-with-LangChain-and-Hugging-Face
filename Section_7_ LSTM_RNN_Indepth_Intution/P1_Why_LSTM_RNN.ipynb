{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999d0fb0",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Long Short-Term Memory (LSTM) and RNN\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Problems with Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a foundational architecture for sequence modeling, but they come with several key limitations:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® 1. Vanishing Gradient Problem\n",
    "\n",
    "- When training RNNs using **Backpropagation Through Time (BPTT)**, gradients are propagated across many time steps.\n",
    "- For long sequences, these gradients **shrink exponentially** (i.e., vanish), making it hard for the network to update earlier weights.\n",
    "- This leads to the model **forgetting early sequence information**, which is critical for many NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš« 2. Exploding Gradients\n",
    "\n",
    "- Opposite of vanishing gradients, sometimes gradients **grow exponentially**, causing unstable weight updates.\n",
    "- This leads to **training instability** and **numerical overflow** unless gradient clipping is applied.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  3. Difficulty Capturing Long-Term Dependencies\n",
    "\n",
    "- Standard RNNs are **biased toward recent inputs** because they struggle to retain older context.\n",
    "- They perform well on **short sequences** but fail when information from earlier time steps is crucial to make accurate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒ 4. Sequential Computation\n",
    "\n",
    "- RNNs **process data one step at a time** (i.e., no parallelism).\n",
    "- This makes training **slower**, especially on long sequences compared to models like Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§± 5. Rigid Memory Structure\n",
    "\n",
    "- RNNs **store all memory in the hidden state**, without a structured way to selectively read/write memory.\n",
    "- This makes them less interpretable and flexible for tasks requiring **explicit memory control**.\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ” These challenges led to the development of more advanced architectures like **LSTM** and **GRU**, which address these problems using gated mechanisms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c973995",
   "metadata": {},
   "source": [
    "### ðŸ” Basic RNN Architecture\n",
    "\n",
    "- A standard RNN processes sequences step-by-step using the same cell at each time step.\n",
    "- RNN structure:\n",
    "  - Inputs: `Xt`\n",
    "  - Hidden States: `ht`\n",
    "  - Outputs: `Yt`\n",
    "- Limitation:\n",
    "  - Cannot retain context over long sequences (e.g., connecting â€œIndiaâ€ to â€œHindiâ€ several steps later).\n",
    "\n",
    "\n",
    "  ### ðŸš« Why RNNs Struggle with Long-Term Dependencies\n",
    "\n",
    "Although RNNs theoretically can carry information from the distant past, in practice they **fail to remember information from earlier in the sequence** due to:\n",
    "\n",
    "- **Vanishing Gradient Problem**: During backpropagation, gradients shrink exponentially through time, making earlier weights ineffective.\n",
    "- **Short-Term Memory Bias**: RNNs rely solely on the hidden state, which gets overwritten at every time stepâ€”losing long-term information.\n",
    "\n",
    "ðŸ“Œ _Example_: In the sentence \"I grew up in India. I speak **Hindi**.\", a vanilla RNN may forget the word \"India\" by the time it processes \"speak\".\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¬ How LSTM Solves This Problem\n",
    "\n",
    "### ðŸ’¡ Long Short-Term Memory (LSTM) Architecture\n",
    "\n",
    "LSTMs introduce a **Cell State (`Câ‚œ`)** in addition to the hidden state. This cell state is like a **memory conveyor belt** that runs through the entire sequence with only minor linear interactionsâ€”making it easier to carry long-term information.\n",
    "\n",
    "LSTM uses **gates** to control the flow of information:\n",
    "\n",
    "- **Forget Gate (`fâ‚œ`)**: Decides what information to discard from the cell state.\n",
    "- **Input Gate (`iâ‚œ`)**: Determines what new information to store.\n",
    "- **Candidate Memory (`Ä‰â‚œ`)**: New potential content to be added to the memory.\n",
    "- **Output Gate (`oâ‚œ`)**: Controls what to output from the current cell.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Cell State: Controlled Memory Flow\n",
    "\n",
    "The **cell state (`Câ‚œ`)** is the key innovation in LSTMs:\n",
    "\n",
    "- âœ… **Selective Retention**: If `fâ‚œ` is close to 1, previous memory is retained.\n",
    "- ðŸ§½ **Forgetting Irrelevant Info**: If `fâ‚œ` is close to 0, previous memory is erased.\n",
    "- âœï¸ **Writing New Info**: `iâ‚œ Ã— Ä‰â‚œ` decides what new content is added to the memory.\n",
    "- ðŸ“¤ **Output**: Final hidden state is derived as `hâ‚œ = oâ‚œ Ã— tanh(Câ‚œ)`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163b590",
   "metadata": {},
   "source": [
    "### ðŸ§® LSTM Mathematical Formulation\n",
    "\n",
    "- **Forget Gate**:  \n",
    "  `ft = Ïƒ(Wf Â· [ht-1, Xt] + bf)`\n",
    "\n",
    "- **Input Gate**:  \n",
    "  `it = Ïƒ(Wi Â· [ht-1, Xt] + bi)`\n",
    "\n",
    "- **Candidate Memory**:  \n",
    "  `CÌƒt = tanh(Wc Â· [ht-1, Xt] + bc)`\n",
    "\n",
    "- **Cell State Update**:  \n",
    "  `Ct = ft * Ct-1 + it * CÌƒt`\n",
    "\n",
    "- **Output Gate**:  \n",
    "  `ot = Ïƒ(Wo Â· [ht-1, Xt] + bo)`\n",
    "\n",
    "- **Hidden State**:  \n",
    "  `ht = ot * tanh(Ct)`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d381c3c",
   "metadata": {},
   "source": [
    "### ðŸ§  Memory Mechanism\n",
    "\n",
    "- **Short-Term Memory** = Hidden state `ht`\n",
    "- **Long-Term Memory** = Cell state `Ct`\n",
    "- LSTM decides:\n",
    "  - What to forget\n",
    "  - What to add\n",
    "  - What to output\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa77d6",
   "metadata": {},
   "source": [
    "### ðŸ§ª Example\n",
    "\n",
    "> â€œI grew up in Indiaâ€¦ I speak ___â€  \n",
    "> RNN may forget \"India\"  \n",
    "> LSTM retains it and predicts a relevant output like \"Hindi\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fe086",
   "metadata": {},
   "source": [
    "\n",
    "### âœ… Summary\n",
    "\n",
    "- LSTM solves RNNâ€™s **long-term dependency** problem.\n",
    "- Uses gated mechanisms to control information flow.\n",
    "- Widely used in NLP for:\n",
    "  - Language modeling\n",
    "  - Machine translation\n",
    "  - Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0a79c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
