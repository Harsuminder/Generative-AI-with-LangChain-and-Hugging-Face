{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999d0fb0",
   "metadata": {},
   "source": [
    "## 📘 Long Short-Term Memory (LSTM) and RNN\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Problems with Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a foundational architecture for sequence modeling, but they come with several key limitations:\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 1. Vanishing Gradient Problem\n",
    "\n",
    "- When training RNNs using **Backpropagation Through Time (BPTT)**, gradients are propagated across many time steps.\n",
    "- For long sequences, these gradients **shrink exponentially** (i.e., vanish), making it hard for the network to update earlier weights.\n",
    "- This leads to the model **forgetting early sequence information**, which is critical for many NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚫 2. Exploding Gradients\n",
    "\n",
    "- Opposite of vanishing gradients, sometimes gradients **grow exponentially**, causing unstable weight updates.\n",
    "- This leads to **training instability** and **numerical overflow** unless gradient clipping is applied.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 3. Difficulty Capturing Long-Term Dependencies\n",
    "\n",
    "- Standard RNNs are **biased toward recent inputs** because they struggle to retain older context.\n",
    "- They perform well on **short sequences** but fail when information from earlier time steps is crucial to make accurate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 🐌 4. Sequential Computation\n",
    "\n",
    "- RNNs **process data one step at a time** (i.e., no parallelism).\n",
    "- This makes training **slower**, especially on long sequences compared to models like Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 5. Rigid Memory Structure\n",
    "\n",
    "- RNNs **store all memory in the hidden state**, without a structured way to selectively read/write memory.\n",
    "- This makes them less interpretable and flexible for tasks requiring **explicit memory control**.\n",
    "\n",
    "---\n",
    "\n",
    "> 🔍 These challenges led to the development of more advanced architectures like **LSTM** and **GRU**, which address these problems using gated mechanisms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c973995",
   "metadata": {},
   "source": [
    "### 🔁 Basic RNN Architecture\n",
    "\n",
    "- A standard RNN processes sequences step-by-step using the same cell at each time step.\n",
    "- RNN structure:\n",
    "  - Inputs: `Xt`\n",
    "  - Hidden States: `ht`\n",
    "  - Outputs: `Yt`\n",
    "- Limitation:\n",
    "  - Cannot retain context over long sequences (e.g., connecting “India” to “Hindi” several steps later).\n",
    "\n",
    "\n",
    "  ### 🚫 Why RNNs Struggle with Long-Term Dependencies\n",
    "\n",
    "Although RNNs theoretically can carry information from the distant past, in practice they **fail to remember information from earlier in the sequence** due to:\n",
    "\n",
    "- **Vanishing Gradient Problem**: During backpropagation, gradients shrink exponentially through time, making earlier weights ineffective.\n",
    "- **Short-Term Memory Bias**: RNNs rely solely on the hidden state, which gets overwritten at every time step—losing long-term information.\n",
    "\n",
    "📌 _Example_: In the sentence \"I grew up in India. I speak **Hindi**.\", a vanilla RNN may forget the word \"India\" by the time it processes \"speak\".\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 How LSTM Solves This Problem\n",
    "\n",
    "### 💡 Long Short-Term Memory (LSTM) Architecture\n",
    "\n",
    "LSTMs introduce a **Cell State (`Cₜ`)** in addition to the hidden state. This cell state is like a **memory conveyor belt** that runs through the entire sequence with only minor linear interactions—making it easier to carry long-term information.\n",
    "\n",
    "LSTM uses **gates** to control the flow of information:\n",
    "\n",
    "- **Forget Gate (`fₜ`)**: Decides what information to discard from the cell state.\n",
    "- **Input Gate (`iₜ`)**: Determines what new information to store.\n",
    "- **Candidate Memory (`ĉₜ`)**: New potential content to be added to the memory.\n",
    "- **Output Gate (`oₜ`)**: Controls what to output from the current cell.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Cell State: Controlled Memory Flow\n",
    "\n",
    "The **cell state (`Cₜ`)** is the key innovation in LSTMs:\n",
    "\n",
    "- ✅ **Selective Retention**: If `fₜ` is close to 1, previous memory is retained.\n",
    "- 🧽 **Forgetting Irrelevant Info**: If `fₜ` is close to 0, previous memory is erased.\n",
    "- ✍️ **Writing New Info**: `iₜ × ĉₜ` decides what new content is added to the memory.\n",
    "- 📤 **Output**: Final hidden state is derived as `hₜ = oₜ × tanh(Cₜ)`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163b590",
   "metadata": {},
   "source": [
    "### 🧮 LSTM Mathematical Formulation\n",
    "\n",
    "- **Forget Gate**:  \n",
    "  `ft = σ(Wf · [ht-1, Xt] + bf)`\n",
    "\n",
    "- **Input Gate**:  \n",
    "  `it = σ(Wi · [ht-1, Xt] + bi)`\n",
    "\n",
    "- **Candidate Memory**:  \n",
    "  `C̃t = tanh(Wc · [ht-1, Xt] + bc)`\n",
    "\n",
    "- **Cell State Update**:  \n",
    "  `Ct = ft * Ct-1 + it * C̃t`\n",
    "\n",
    "- **Output Gate**:  \n",
    "  `ot = σ(Wo · [ht-1, Xt] + bo)`\n",
    "\n",
    "- **Hidden State**:  \n",
    "  `ht = ot * tanh(Ct)`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d381c3c",
   "metadata": {},
   "source": [
    "### 🧠 Memory Mechanism\n",
    "\n",
    "- **Short-Term Memory** = Hidden state `ht`\n",
    "- **Long-Term Memory** = Cell state `Ct`\n",
    "- LSTM decides:\n",
    "  - What to forget\n",
    "  - What to add\n",
    "  - What to output\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa77d6",
   "metadata": {},
   "source": [
    "### 🧪 Example\n",
    "\n",
    "> “I grew up in India… I speak ___”  \n",
    "> RNN may forget \"India\"  \n",
    "> LSTM retains it and predicts a relevant output like \"Hindi\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fe086",
   "metadata": {},
   "source": [
    "\n",
    "### ✅ Summary\n",
    "\n",
    "- LSTM solves RNN’s **long-term dependency** problem.\n",
    "- Uses gated mechanisms to control information flow.\n",
    "- Widely used in NLP for:\n",
    "  - Language modeling\n",
    "  - Machine translation\n",
    "  - Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0a79c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
