{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a774808a",
   "metadata": {},
   "source": [
    "## 📘 Long Short-Term Memory (LSTM RNN) and Recurrent Neural Networks (RNN)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Problems with Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are widely used for sequence modeling tasks like language modeling, time-series forecasting, and sentiment analysis. However, they suffer from several critical limitations:\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 1. Vanishing Gradient Problem\n",
    "- During backpropagation through time (BPTT), gradients often shrink exponentially across long sequences.\n",
    "- As a result, earlier layers receive negligible updates, making it hard to learn dependencies from earlier time steps.\n",
    "- This causes RNNs to \"forget\" long-term context.\n",
    "\n",
    "---\n",
    "\n",
    "### 💥 2. Exploding Gradients\n",
    "- Sometimes, gradients grow exponentially instead of shrinking.\n",
    "- This leads to very large weight updates, instability, and overflow issues during training.\n",
    "- Requires techniques like **gradient clipping** to mitigate.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 3. Difficulty Capturing Long-Term Dependencies\n",
    "- Standard RNNs are biased towards recent inputs.\n",
    "- They struggle to capture dependencies when important information is several steps away in the sequence.\n",
    "\n",
    "📌 **Example**:  \n",
    "> \"I grew up in India. I speak ___\"  \n",
    "> A vanilla RNN may forget \"India\" before predicting \"Hindi\".\n",
    "\n",
    "---\n",
    "\n",
    "### 🐌 4. Sequential Computation Bottleneck\n",
    "- RNNs process inputs step-by-step, making them inherently slow.\n",
    "- Lack of parallelism limits training efficiency on long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 5. Rigid Memory Structure\n",
    "- All memory is stored in a single hidden state.\n",
    "- No structured mechanism to retain, forget, or retrieve specific information.\n",
    "\n",
    "---\n",
    "\n",
    "> 🔍 These limitations motivated the development of improved architectures like **LSTM RNN** and **GRU**, which introduced gated memory control mechanisms.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Basic RNN Architecture\n",
    "\n",
    "- An RNN consists of a repeating module applied to each time step in the input sequence.\n",
    "- Each module takes:\n",
    "  - Current input `Xt`\n",
    "  - Previous hidden state `ht-1`\n",
    "  - Produces new hidden state `ht`\n",
    "  \n",
    "However, the hidden state gets overwritten at every step — causing memory loss over time.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚫 Why RNNs Struggle with Long-Term Dependencies\n",
    "\n",
    "Although RNNs theoretically can capture long-range patterns, they often fail in practice due to:\n",
    "- **Vanishing gradients**\n",
    "- **Overwritten hidden state at every time step**\n",
    "  \n",
    "This leads to a **short-term memory bias**, making them poor at tasks that require long-term context.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 How LSTM RNN Solves the Problem\n",
    "\n",
    "LSTM RNNs (Long Short-Term Memory Recurrent Neural Networks) were specifically designed to handle long-term dependencies in sequential data.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 LSTM RNN Architecture Overview\n",
    "\n",
    "LSTM RNN introduces a new component: **Cell State (`Ct`)**, which acts as a conveyor belt carrying information across time steps with minimal modification.\n",
    "\n",
    "LSTM RNN uses **gates** to control the flow of information:\n",
    "\n",
    "- **Forget Gate**: `ft = σ(Wf · [ht-1, Xt] + bf)`  \n",
    "  Decides what to forget from the cell state.\n",
    "\n",
    "- **Input Gate**: `it = σ(Wi · [ht-1, Xt] + bi)`  \n",
    "  Determines what new information to add.\n",
    "\n",
    "- **Candidate Memory**: `C̃t = tanh(Wc · [ht-1, Xt] + bc)`  \n",
    "  New content to potentially store.\n",
    "\n",
    "- **Cell State Update**: `Ct = ft * Ct-1 + it * C̃t`  \n",
    "  Final cell state after combining forget and input flows.\n",
    "\n",
    "- **Output Gate**: `ot = σ(Wo · [ht-1, Xt] + bo)`  \n",
    "  Controls what part of the cell state is output.\n",
    "\n",
    "- **Hidden State**: `ht = ot * tanh(Ct)`\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Memory Mechanism in LSTM RNN\n",
    "\n",
    "- **Short-Term Memory** is stored in `ht` (hidden state).\n",
    "- **Long-Term Memory** is maintained in `Ct` (cell state).\n",
    "- LSTM RNN can:\n",
    "  - ✅ Retain important info (`ft` ≈ 1)\n",
    "  - 🧽 Forget irrelevant info (`ft` ≈ 0)\n",
    "  - ✍️ Add new info via input gate\n",
    "  - 📤 Output relevant info via output gate\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Illustrative Example\n",
    "\n",
    "Sentence:  \n",
    "> \"I grew up in India. I speak ___\"\n",
    "\n",
    "- A basic RNN may forget \"India\" when reaching \"speak\".\n",
    "- An LSTM RNN retains that context in the **cell state**, improving the prediction of \"Hindi\".\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- RNNs are limited by vanishing gradients, short-term memory bias, and sequential bottlenecks.\n",
    "- LSTM RNN solves these issues by introducing:\n",
    "  - A dedicated **cell state** for long-term memory.\n",
    "  - **Gated mechanisms** to regulate memory flow.\n",
    "- LSTM RNN is the foundation for modern NLP architectures and is widely used in:\n",
    "  - Sentiment analysis\n",
    "  - Language modeling\n",
    "  - Text generation\n",
    "  - Machine translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163b590",
   "metadata": {},
   "source": [
    "### 🧮 LSTM Mathematical Formulation\n",
    "\n",
    "- **Forget Gate**:  \n",
    "  `ft = σ(Wf · [ht-1, Xt] + bf)`\n",
    "\n",
    "- **Input Gate**:  \n",
    "  `it = σ(Wi · [ht-1, Xt] + bi)`\n",
    "\n",
    "- **Candidate Memory**:  \n",
    "  `C̃t = tanh(Wc · [ht-1, Xt] + bc)`\n",
    "\n",
    "- **Cell State Update**:  \n",
    "  `Ct = ft * Ct-1 + it * C̃t`\n",
    "\n",
    "- **Output Gate**:  \n",
    "  `ot = σ(Wo · [ht-1, Xt] + bo)`\n",
    "\n",
    "- **Hidden State**:  \n",
    "  `ht = ot * tanh(Ct)`\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
