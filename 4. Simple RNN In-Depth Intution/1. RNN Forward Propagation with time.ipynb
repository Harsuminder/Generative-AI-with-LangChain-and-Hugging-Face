{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d8540f",
   "metadata": {},
   "source": [
    "## 🔁 RNN Forward Propagation Through Time – Conceptual Walkthrough\n",
    "\n",
    "A **Recurrent Neural Network (RNN)** processes sequential data one step at a time, maintaining memory of previous inputs using a hidden state. This is ideal for NLP tasks where **word order and context matter**, such as text generation, translation, and classification.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Example: Three Simple Sentences\n",
    "\n",
    "We’ll build a vocabulary and simulate RNN forward propagation using the following toy dataset:\n",
    "\n",
    "Sentence 1: I like NLP  \n",
    "Sentence 2: You study hard  \n",
    "Sentence 3: We write code\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 Vocabulary\n",
    "\n",
    "[\"I\", \"like\", \"NLP\", \"You\", \"study\", \"hard\", \"We\", \"write\", \"code\"]\n",
    "\n",
    "Vocabulary size = **9** (Each word is unique)  \n",
    "We’ll represent each word using **One-Hot Encoding**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔤 One-Hot Encoded Vectors for Sentence 1: \"I like NLP\"\n",
    "\n",
    "| Word   | One-Hot Vector             |\n",
    "|--------|-----------------------------|\n",
    "| I      | [1 0 0 0 0 0 0 0 0]         |\n",
    "| like   | [0 1 0 0 0 0 0 0 0]         |\n",
    "| NLP    | [0 0 1 0 0 0 0 0 0]         |\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱ Forward Propagation Through Time (Step-by-Step)\n",
    "\n",
    "We feed **one word at a time** into the RNN.\n",
    "\n",
    "Let’s denote:  \n",
    "- `x11`, `x12`, `x13` as the word vectors at time steps t = 1, 2, 3  \n",
    "- `w` as the **input-to-hidden** weights  \n",
    "- `w'` as the **hidden-to-hidden** weights  \n",
    "- `b` as the bias for hidden state  \n",
    "- `f()` as an activation function (like tanh or ReLU)  \n",
    "- `h1`, `h2`, `h3` as the hidden states  \n",
    "- `ŷ` as the final prediction  \n",
    "- `L` as the loss function\n",
    "\n",
    "---\n",
    "\n",
    "           ┌───────────────┐  \n",
    "x11 ─────> │   RNN Cell    │  \n",
    "           └──────┬────────┘  \n",
    "                  ↓  \n",
    "                h1  \n",
    "                  ↓  \n",
    "x12 ─────> ┌──────┴──────┐  \n",
    "           │   RNN Cell  │  \n",
    "           └──────┬──────┘  \n",
    "                  ↓  \n",
    "                h2  \n",
    "                  ↓  \n",
    "x13 ─────> ┌──────┴──────┐  \n",
    "           │   RNN Cell  │  \n",
    "           └──────┬──────┘  \n",
    "                  ↓  \n",
    "                h3 ─────> ŷ (Prediction)\n",
    "---\n",
    "\n",
    "### 🧮 Forward Pass Equations\n",
    "\n",
    "**Time Step 1 (t = 1)**  \n",
    "Input: x11 (word \"I\")\n",
    "\n",
    "h1 = f(x11 ⋅ w + b)\n",
    "\n",
    "---\n",
    "\n",
    "**Time Step 2 (t = 2)**  \n",
    "Input: x12 (word \"like\")\n",
    "\n",
    "h2 = f(x12 ⋅ w + h1 ⋅ w' + b)\n",
    "\n",
    "---\n",
    "\n",
    "**Time Step 3 (t = 3)**  \n",
    "Input: x13 (word \"NLP\")\n",
    "\n",
    "h3 = f(x13 ⋅ w + h2 ⋅ w' + b)\n",
    "\n",
    "---\n",
    "\n",
    "### 📤 Output and Loss Computation\n",
    "\n",
    "ŷ = softmax(h3 ⋅ v + b_output)\n",
    "\n",
    "Loss = y- ŷ\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Backpropagation Through Time (BPTT)\n",
    "\n",
    "- Loss is propagated backward through all time steps  \n",
    "- Gradients are calculated for all trainable parameters (`w`, `w'`, `v`, `b`, `b_output`)  \n",
    "- Parameters are updated using gradient descent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e971b9f",
   "metadata": {},
   "source": [
    "### 🔢 Parameter Summary Example\n",
    "\n",
    "Assume:  \n",
    "- Input size (vocabulary size) = 9  \n",
    "- Hidden size = 5  \n",
    "- Output size = 3  \n",
    "\n",
    "| Component           | Shape         | Parameters |\n",
    "|---------------------|---------------|------------|\n",
    "| Input → Hidden      | 9 × 5         | 45         |\n",
    "| Hidden → Hidden     | 5 × 5         | 25         |\n",
    "| Hidden → Output     | 5 × 3         | 15         |\n",
    "| Biases              | 5 (hidden) + 3 (output) | 8 |\n",
    "\n",
    "*Total Trainable Parameters = 45 + 25 + 15 + 8 = 93*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df084e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
