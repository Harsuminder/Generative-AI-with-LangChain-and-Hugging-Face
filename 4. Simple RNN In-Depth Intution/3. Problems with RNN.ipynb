{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e992642f",
   "metadata": {},
   "source": [
    "## ⚠️ Problems with Recurrent Neural Networks (RNNs)\n",
    "\n",
    "While RNNs are powerful for handling sequential data like text, speech, or time series, they suffer from several key limitations:\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Vanishing Gradient Problem\n",
    "\n",
    "- During **Backpropagation Through Time (BPTT)**, gradients are multiplied at each time step.\n",
    "- With long sequences, these gradients can **shrink exponentially**.\n",
    "- Eventually, they become **too small to make updates**, especially for earlier layers.\n",
    "- ➡️ Result: The network **fails to learn long-term dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Exploding Gradient Problem\n",
    "\n",
    "- Opposite of vanishing: gradients **grow exponentially** as they flow backward.\n",
    "- Can lead to **numerical instability**, model divergence, or NaN errors.\n",
    "- ➡️ Often requires **gradient clipping** to fix.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Difficulty Learning Long-Term Dependencies\n",
    "\n",
    "- RNNs **struggle with remembering context** from many time steps ago.\n",
    "- Works fine for **short sequences**, but fails to model **long-range context** (e.g., in long sentences or documents).\n",
    "- ➡️ This is a major motivation for **LSTM** and **GRU** architectures.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Sequential Computation (Slow Training)\n",
    "\n",
    "- RNNs process input **one time step at a time**.\n",
    "- ➡️ Cannot be parallelized easily → **slow training**, especially on long sequences or large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Fixed-Length Hidden State\n",
    "\n",
    "- The entire context is compressed into a single hidden state vector.\n",
    "- ➡️ Not ideal for very complex or information-rich sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### 6️⃣ Bias Toward Recent Inputs\n",
    "\n",
    "- Since recent inputs are closer to the final output, they tend to have **more influence** during training.\n",
    "- ➡️ The model may ignore earlier parts of the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary Table\n",
    "\n",
    "| Issue                        | Cause                           | Effect                                 | Fix                      |\n",
    "|-----------------------------|----------------------------------|----------------------------------------|--------------------------|\n",
    "| Vanishing gradients          | Small derivatives in BPTT       | No learning for early time steps       | LSTM/GRU, ReLU, LayerNorm|\n",
    "| Exploding gradients          | Large derivatives in BPTT       | Instability, NaN                       | Gradient clipping        |\n",
    "| Forgetting long-term context| Limited memory capacity          | Can't model long dependencies          | LSTM/GRU                 |\n",
    "| Slow training                | Step-by-step computation        | Low parallelism                        | Transformer (parallel)   |\n",
    "| Fixed context vector         | Hidden state bottleneck         | Loss of semantic richness              | Attention mechanism      |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356cb93b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
