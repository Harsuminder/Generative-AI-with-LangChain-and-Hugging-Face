{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c114f13f",
   "metadata": {},
   "source": [
    "# 🔄 Backpropagation Through Time (BPTT) – RNN Training Explained\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What is Backpropagation Through Time?\n",
    "\n",
    "In a Recurrent Neural Network (RNN), we process input **step by step**, using hidden states to carry forward **contextual memory**.  \n",
    "After computing the final output and **loss**, the model must **update its parameters** to improve performance — this happens through **Backpropagation Through Time (BPTT)**.\n",
    "\n",
    "BPTT is an extension of backpropagation used for **sequential models**, where **gradients flow backward not just through layers**, but also **across time steps**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Example Recap: Sentence \"I like NLP\"\n",
    "\n",
    "Let’s revisit our toy sentence and its one-hot encoded inputs:\n",
    "\n",
    "**Sentence:**  \n",
    "\"I like NLP\"\n",
    "\n",
    "**Time steps:**\n",
    "- t=1 → x₁₁ = \"I\"\n",
    "- t=2 → x₁₂ = \"like\"\n",
    "- t=3 → x₁₃ = \"NLP\"\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱ Forward Pass (from before)\n",
    "\n",
    "At each time step, the RNN computes the hidden state:\n",
    "\n",
    "- h₁ = f(x₁₁ ⋅ W + b)  \n",
    "- h₂ = f(x₁₂ ⋅ W + h₁ ⋅ Wʰ + b)  \n",
    "- h₃ = f(x₁₃ ⋅ W + h₂ ⋅ Wʰ + b)\n",
    "\n",
    "Final output:\n",
    "- ŷ = softmax(h₃ ⋅ V + b_output)\n",
    "\n",
    "Loss:\n",
    "- L = ŷ - y  ← (difference between predicted and actual target)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Backpropagation Through Time Begins\n",
    "\n",
    "BPTT computes **gradients of the loss with respect to all weights**, including those used **at every time step**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Gradient Flow Overview\n",
    "\n",
    "We compute partial derivatives and update:\n",
    "\n",
    "1. **dL/dŷ** ← gradient of loss w.r.t output  \n",
    "2. **Backprop through output layer**:  \n",
    "   - Update `V` and `b_output`  \n",
    "   - Formula:  \n",
    "     - ∂L/∂V = h₃ᵗ × (ŷ - y)  \n",
    "     - V ← V - α × ∂L/∂V  \n",
    "\n",
    "3. **Backprop through hidden states** (from h₃ → h₂ → h₁):\n",
    "   - Accumulate gradients due to recurrence\n",
    "   - Update `Wʰ` (shared at all time steps)\n",
    "   - Update `W` (input-to-hidden for x₁₁, x₁₂, x₁₃)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Parameter Update – Chain Rule and Weight Flow\n",
    "\n",
    "At each time step (starting from last):\n",
    "\n",
    "### 📌 Update `V` (Hidden → Output)\n",
    "- `ŷ` depends on `o₃ = h₃ ⋅ V`\n",
    "- Update rule:  \n",
    "V ← V - α × ∂L/∂V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0a848",
   "metadata": {},
   "source": [
    "\n",
    "Where:\n",
    "- ∂Lₜ/∂Wʰ = ∂Lₜ/∂hₜ × ∂hₜ/∂hₜ₋₁ × ∂hₜ₋₁/∂Wʰ\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Update `W` (Input → Hidden)\n",
    "\n",
    "Each input `x₁₁`, `x₁₂`, `x₁₃` contributes separately:\n",
    "W ← W - α × (∂L₁/∂W + ∂L₂/∂W + ∂L₃/∂W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7626c",
   "metadata": {},
   "source": [
    "\n",
    "Use chain rule:\n",
    "- ∂Lₜ/∂W = ∂Lₜ/∂hₜ × ∂hₜ/∂xₜ × ∂xₜ/∂W\n",
    "\n",
    "---\n",
    "\n",
    "## 🔃 One Round of BPTT Summary\n",
    "\n",
    "For a 3-word sentence, backpropagation flows like this:\n",
    "\n",
    "- Time t = 3  \n",
    "  - Gradients flow: ŷ → o₃ → h₃ → h₂ → h₁  \n",
    "- Time t = 2  \n",
    "  - h₂ influences future loss → gradient flows to h₂  \n",
    "- Time t = 1  \n",
    "  - h₁ is updated based on indirect impact on h₂ and h₃\n",
    "\n",
    "All updates are **added across time** before applying gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 Final Update Formulas (Gradient Descent)\n",
    "\n",
    "- `V ← V - α × ∂L/∂V`  \n",
    "- `W ← W - α × (∂L₁/∂W + ∂L₂/∂W + ∂L₃/∂W)`  \n",
    "- `Wʰ ← Wʰ - α × (∂L₁/∂Wʰ + ∂L₂/∂Wʰ + ∂L₃/∂Wʰ)`  \n",
    "\n",
    "Where `α` is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Example Learning Rate and Intuition\n",
    "\n",
    "- Typical value: α = 0.0001 or 0.001  \n",
    "- A small α ensures **slow but stable** learning  \n",
    "- Too high → unstable gradients  \n",
    "- Too low → very slow training\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Parameters Getting Updated\n",
    "\n",
    "| Parameter         | Used At               | Updated In BPTT |\n",
    "|------------------|------------------------|------------------|\n",
    "| W (Input → Hidden)   | All time steps         | ✅ Yes            |\n",
    "| Wʰ (Hidden → Hidden) | All time steps         | ✅ Yes            |\n",
    "| V (Hidden → Output)  | Final output layer     | ✅ Yes            |\n",
    "| b (Hidden bias)      | All time steps         | ✅ Yes            |\n",
    "| b_output             | Output layer           | ✅ Yes            |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc3192",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ Recap\n",
    "\n",
    "- Forward pass: one word at a time → hidden states → final prediction\n",
    "- Backpropagation through time flows **backward across time**\n",
    "- Uses **chain rule** to update shared weights\n",
    "- Loss reduction occurs via repeated forward + backward passes until convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db002c5f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
