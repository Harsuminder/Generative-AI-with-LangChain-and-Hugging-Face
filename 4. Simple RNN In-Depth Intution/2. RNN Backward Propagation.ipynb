{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e77d07",
   "metadata": {},
   "source": [
    "# 🔁 Backpropagation Through Time (BPTT) – In-Depth Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What Happens During BTT?\n",
    "\n",
    "When training an RNN, we perform **forward propagation through time** to calculate the output `ŷ` and compute the **loss**.\n",
    "\n",
    "Then, to improve the model, we perform **Backpropagation Through Time (BPTT)** — where we compute the gradients of the loss **with respect to all parameters** and update them using gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What Parameters Get Updated?\n",
    "\n",
    "During BTT, we update the following:\n",
    "\n",
    "- **W** → input-to-hidden weights  \n",
    "- **Wʰ** → hidden-to-hidden weights (shared across time)  \n",
    "- **V** → hidden-to-output weights  \n",
    "- **b** → bias for hidden layer  \n",
    "- **b_output** → bias for output layer\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Gradient Flow using Chain Rule\n",
    "\n",
    "Let’s say we process an input sequence across 3 time steps:\n",
    "- t = 1 → `x₁`\n",
    "- t = 2 → `x₂`\n",
    "- t = 3 → `x₃`\n",
    "\n",
    "And we compute:\n",
    "- `h₁ = f(x₁ ⋅ W + b)`\n",
    "- `h₂ = f(x₂ ⋅ W + h₁ ⋅ Wʰ + b)`\n",
    "- `h₃ = f(x₃ ⋅ W + h₂ ⋅ Wʰ + b)`\n",
    "- `ŷ = softmax(h₃ ⋅ V + b_output)`\n",
    "- `L = loss(ŷ, y)`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6eab7",
   "metadata": {},
   "source": [
    "    Time Steps →         t = 1             t = 2               t = 3\n",
    "\n",
    "  Input x₁ₜ         →   [ x₁₁ ]           [ x₁₂ ]               [ x₁₃ ]\n",
    "                       (input vector)    (input vector)     (input vector)\n",
    "\n",
    "                          │                  │                  │\n",
    "                         [W]                [W]                [W]\n",
    "                          │                  │                  │\n",
    "                        ┌────┐            ┌────┐            ┌────┐\n",
    "     Hidden state hₜ →   │ h₁ │ ───[Wʰ]── →│ h₂ │ ───[Wʰ]───→│ h₃ │\n",
    "                        └────┘            └────┘            └────┘\n",
    "                                                               │\n",
    "                                                              [V]\n",
    "                                                               │\n",
    "                                                       Output ŷ (softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba1dc5",
   "metadata": {},
   "source": [
    "---\n",
    "## 🔧 Step 1: Update Output Layer Weights (V)\n",
    "\n",
    "V is updated using formula:\n",
    "\n",
    "V_new = V_old - η ⋅ ∂L/∂V_old\n",
    "\n",
    "where - `η` is the **learning rate**\n",
    "- Controls **how big a step** we take in the direction of the gradient\n",
    "- A small `η` (e.g., 0.0001) means slow learning; large `η` can lead to overshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032b8bc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔧 Step 2: Update Hidden-to-Hidden Weights (Wʰ)\n",
    "\n",
    "Wʰ is **reused across time steps**, so its gradient is the **sum of contributions from each time step**.\n",
    "\n",
    "So, here we calculate:\n",
    "\n",
    "Wʰ_new = Wʰ_old - η ⋅ ∂L/∂Wʰ_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838d404",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔧 Step 3: Update Input-to-Hidden Weights (W)\n",
    "\n",
    "Similar to Wʰ, we use formula:\n",
    "\n",
    "W_new = W_old - η ⋅ ∂L/∂W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Summary of BTT Steps\n",
    "\n",
    "1. Perform **forward pass** to compute all `hₜ`, final output `ŷ`, and loss `L`\n",
    "2. Compute gradients of `L` w.r.t.:\n",
    "   - Output layer weights `V`\n",
    "   - Hidden-to-hidden weights `Wʰ` (shared across time)\n",
    "   - Input-to-hidden weights `W`\n",
    "   - Biases\n",
    "3. Use **chain rule** to propagate loss **back through time**\n",
    "4. Update all weights using **gradient descent**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Reminder: This Happens for Every Sequence During Training\n",
    "\n",
    "The above gradient flow happens for **each training sample (sequence)** during every epoch.\n",
    "\n",
    "Weights get updated → Output improves → Loss reduces → Model learns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7712349c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
