{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e77d07",
   "metadata": {},
   "source": [
    "# ğŸ” Backpropagation Through Time (BPTT) â€“ In-Depth Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ What Happens During BTT?\n",
    "\n",
    "When training an RNN, we perform **forward propagation through time** to calculate the output `Å·` and compute the **loss**.\n",
    "\n",
    "Then, to improve the model, we perform **Backpropagation Through Time (BPTT)** â€” where we compute the gradients of the loss **with respect to all parameters** and update them using gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What Parameters Get Updated?\n",
    "\n",
    "During BTT, we update the following:\n",
    "\n",
    "- **W** â†’ input-to-hidden weights  \n",
    "- **WÊ°** â†’ hidden-to-hidden weights (shared across time)  \n",
    "- **V** â†’ hidden-to-output weights  \n",
    "- **b** â†’ bias for hidden layer  \n",
    "- **b_output** â†’ bias for output layer\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® Gradient Flow using Chain Rule\n",
    "\n",
    "Letâ€™s say we process an input sequence across 3 time steps:\n",
    "- t = 1 â†’ `xâ‚`\n",
    "- t = 2 â†’ `xâ‚‚`\n",
    "- t = 3 â†’ `xâ‚ƒ`\n",
    "\n",
    "And we compute:\n",
    "- `hâ‚ = f(xâ‚ â‹… W + b)`\n",
    "- `hâ‚‚ = f(xâ‚‚ â‹… W + hâ‚ â‹… WÊ° + b)`\n",
    "- `hâ‚ƒ = f(xâ‚ƒ â‹… W + hâ‚‚ â‹… WÊ° + b)`\n",
    "- `Å· = softmax(hâ‚ƒ â‹… V + b_output)`\n",
    "- `L = loss(Å·, y)`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6eab7",
   "metadata": {},
   "source": [
    "    Time Steps â†’         t = 1             t = 2               t = 3\n",
    "\n",
    "  Input xâ‚â‚œ         â†’   [ xâ‚â‚ ]           [ xâ‚â‚‚ ]               [ xâ‚â‚ƒ ]\n",
    "                       (input vector)    (input vector)     (input vector)\n",
    "\n",
    "                          â”‚                  â”‚                  â”‚\n",
    "                         [W]                [W]                [W]\n",
    "                          â”‚                  â”‚                  â”‚\n",
    "                        â”Œâ”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”\n",
    "     Hidden state hâ‚œ â†’   â”‚ hâ‚ â”‚ â”€â”€â”€[WÊ°]â”€â”€ â†’â”‚ hâ‚‚ â”‚ â”€â”€â”€[WÊ°]â”€â”€â”€â†’â”‚ hâ‚ƒ â”‚\n",
    "                        â””â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”˜\n",
    "                                                               â”‚\n",
    "                                                              [V]\n",
    "                                                               â”‚\n",
    "                                                       Output Å· (softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba1dc5",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”§ Step 1: Update Output Layer Weights (V)\n",
    "\n",
    "V is updated using formula:\n",
    "\n",
    "V_new = V_old - Î· â‹… âˆ‚L/âˆ‚V_old\n",
    "\n",
    "where - `Î·` is the **learning rate**\n",
    "- Controls **how big a step** we take in the direction of the gradient\n",
    "- A small `Î·` (e.g., 0.0001) means slow learning; large `Î·` can lead to overshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032b8bc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Step 2: Update Hidden-to-Hidden Weights (WÊ°)\n",
    "\n",
    "WÊ° is **reused across time steps**, so its gradient is the **sum of contributions from each time step**.\n",
    "\n",
    "So, here we calculate:\n",
    "\n",
    "WÊ°_new = WÊ°_old - Î· â‹… âˆ‚L/âˆ‚WÊ°_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838d404",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Step 3: Update Input-to-Hidden Weights (W)\n",
    "\n",
    "Similar to WÊ°, we use formula:\n",
    "\n",
    "W_new = W_old - Î· â‹… âˆ‚L/âˆ‚W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary of BTT Steps\n",
    "\n",
    "1. Perform **forward pass** to compute all `hâ‚œ`, final output `Å·`, and loss `L`\n",
    "2. Compute gradients of `L` w.r.t.:\n",
    "   - Output layer weights `V`\n",
    "   - Hidden-to-hidden weights `WÊ°` (shared across time)\n",
    "   - Input-to-hidden weights `W`\n",
    "   - Biases\n",
    "3. Use **chain rule** to propagate loss **back through time**\n",
    "4. Update all weights using **gradient descent**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Reminder: This Happens for Every Sequence During Training\n",
    "\n",
    "The above gradient flow happens for **each training sample (sequence)** during every epoch.\n",
    "\n",
    "Weights get updated â†’ Output improves â†’ Loss reduces â†’ Model learns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7712349c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
