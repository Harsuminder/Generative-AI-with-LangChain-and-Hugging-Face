{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c114f13f",
   "metadata": {},
   "source": [
    "# ğŸ”„ Backpropagation Through Time (BPTT) â€“ RNN Training Explained\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ What is Backpropagation Through Time?\n",
    "\n",
    "In a Recurrent Neural Network (RNN), we process input **step by step**, using hidden states to carry forward **contextual memory**.  \n",
    "After computing the final output and **loss**, the model must **update its parameters** to improve performance â€” this happens through **Backpropagation Through Time (BPTT)**.\n",
    "\n",
    "BPTT is an extension of backpropagation used for **sequential models**, where **gradients flow backward not just through layers**, but also **across time steps**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Example Recap: Sentence \"I like NLP\"\n",
    "\n",
    "Letâ€™s revisit our toy sentence and its one-hot encoded inputs:\n",
    "\n",
    "**Sentence:**  \n",
    "\"I like NLP\"\n",
    "\n",
    "**Time steps:**\n",
    "- t=1 â†’ xâ‚â‚ = \"I\"\n",
    "- t=2 â†’ xâ‚â‚‚ = \"like\"\n",
    "- t=3 â†’ xâ‚â‚ƒ = \"NLP\"\n",
    "\n",
    "---\n",
    "\n",
    "## â± Forward Pass (from before)\n",
    "\n",
    "At each time step, the RNN computes the hidden state:\n",
    "\n",
    "- hâ‚ = f(xâ‚â‚ â‹… W + b)  \n",
    "- hâ‚‚ = f(xâ‚â‚‚ â‹… W + hâ‚ â‹… WÊ° + b)  \n",
    "- hâ‚ƒ = f(xâ‚â‚ƒ â‹… W + hâ‚‚ â‹… WÊ° + b)\n",
    "\n",
    "Final output:\n",
    "- Å· = softmax(hâ‚ƒ â‹… V + b_output)\n",
    "\n",
    "Loss:\n",
    "- L = yÌ‚ - y  â† (difference between predicted and actual target)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Backpropagation Through Time Begins\n",
    "\n",
    "BPTT computes **gradients of the loss with respect to all weights**, including those used **at every time step**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Gradient Flow Overview\n",
    "\n",
    "We compute partial derivatives and update:\n",
    "\n",
    "1. **dL/dÅ·** â† gradient of loss w.r.t output  \n",
    "2. **Backprop through output layer**:  \n",
    "   - Update `V` and `b_output`  \n",
    "   - Formula:  \n",
    "     - âˆ‚L/âˆ‚V = hâ‚ƒáµ— Ã— (Å· - y)  \n",
    "     - V â† V - Î± Ã— âˆ‚L/âˆ‚V  \n",
    "\n",
    "3. **Backprop through hidden states** (from hâ‚ƒ â†’ hâ‚‚ â†’ hâ‚):\n",
    "   - Accumulate gradients due to recurrence\n",
    "   - Update `WÊ°` (shared at all time steps)\n",
    "   - Update `W` (input-to-hidden for xâ‚â‚, xâ‚â‚‚, xâ‚â‚ƒ)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Parameter Update â€“ Chain Rule and Weight Flow\n",
    "\n",
    "At each time step (starting from last):\n",
    "\n",
    "### ğŸ“Œ Update `V` (Hidden â†’ Output)\n",
    "- `Å·` depends on `oâ‚ƒ = hâ‚ƒ â‹… V`\n",
    "- Update rule:  \n",
    "V â† V - Î± Ã— âˆ‚L/âˆ‚V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0a848",
   "metadata": {},
   "source": [
    "\n",
    "Where:\n",
    "- âˆ‚Lâ‚œ/âˆ‚WÊ° = âˆ‚Lâ‚œ/âˆ‚hâ‚œ Ã— âˆ‚hâ‚œ/âˆ‚hâ‚œâ‚‹â‚ Ã— âˆ‚hâ‚œâ‚‹â‚/âˆ‚WÊ°\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Update `W` (Input â†’ Hidden)\n",
    "\n",
    "Each input `xâ‚â‚`, `xâ‚â‚‚`, `xâ‚â‚ƒ` contributes separately:\n",
    "W â† W - Î± Ã— (âˆ‚Lâ‚/âˆ‚W + âˆ‚Lâ‚‚/âˆ‚W + âˆ‚Lâ‚ƒ/âˆ‚W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7626c",
   "metadata": {},
   "source": [
    "\n",
    "Use chain rule:\n",
    "- âˆ‚Lâ‚œ/âˆ‚W = âˆ‚Lâ‚œ/âˆ‚hâ‚œ Ã— âˆ‚hâ‚œ/âˆ‚xâ‚œ Ã— âˆ‚xâ‚œ/âˆ‚W\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”ƒ One Round of BPTT Summary\n",
    "\n",
    "For a 3-word sentence, backpropagation flows like this:\n",
    "\n",
    "- Time t = 3  \n",
    "  - Gradients flow: Å· â†’ oâ‚ƒ â†’ hâ‚ƒ â†’ hâ‚‚ â†’ hâ‚  \n",
    "- Time t = 2  \n",
    "  - hâ‚‚ influences future loss â†’ gradient flows to hâ‚‚  \n",
    "- Time t = 1  \n",
    "  - hâ‚ is updated based on indirect impact on hâ‚‚ and hâ‚ƒ\n",
    "\n",
    "All updates are **added across time** before applying gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¾ Final Update Formulas (Gradient Descent)\n",
    "\n",
    "- `V â† V - Î± Ã— âˆ‚L/âˆ‚V`  \n",
    "- `W â† W - Î± Ã— (âˆ‚Lâ‚/âˆ‚W + âˆ‚Lâ‚‚/âˆ‚W + âˆ‚Lâ‚ƒ/âˆ‚W)`  \n",
    "- `WÊ° â† WÊ° - Î± Ã— (âˆ‚Lâ‚/âˆ‚WÊ° + âˆ‚Lâ‚‚/âˆ‚WÊ° + âˆ‚Lâ‚ƒ/âˆ‚WÊ°)`  \n",
    "\n",
    "Where `Î±` is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ Example Learning Rate and Intuition\n",
    "\n",
    "- Typical value: Î± = 0.0001 or 0.001  \n",
    "- A small Î± ensures **slow but stable** learning  \n",
    "- Too high â†’ unstable gradients  \n",
    "- Too low â†’ very slow training\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Parameters Getting Updated\n",
    "\n",
    "| Parameter         | Used At               | Updated In BPTT |\n",
    "|------------------|------------------------|------------------|\n",
    "| W (Input â†’ Hidden)   | All time steps         | âœ… Yes            |\n",
    "| WÊ° (Hidden â†’ Hidden) | All time steps         | âœ… Yes            |\n",
    "| V (Hidden â†’ Output)  | Final output layer     | âœ… Yes            |\n",
    "| b (Hidden bias)      | All time steps         | âœ… Yes            |\n",
    "| b_output             | Output layer           | âœ… Yes            |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc3192",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## âœ… Recap\n",
    "\n",
    "- Forward pass: one word at a time â†’ hidden states â†’ final prediction\n",
    "- Backpropagation through time flows **backward across time**\n",
    "- Uses **chain rule** to update shared weights\n",
    "- Loss reduction occurs via repeated forward + backward passes until convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db002c5f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
