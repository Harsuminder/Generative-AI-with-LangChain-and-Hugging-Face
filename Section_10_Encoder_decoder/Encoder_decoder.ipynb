{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1b15fc",
   "metadata": {},
   "source": [
    "## ğŸ” Quick Recap: RNNs So Far\n",
    "\n",
    "Before diving into **Encoder-Decoder** models, letâ€™s revisit what weâ€™ve learned:\n",
    "\n",
    "- **Simple RNN**: Introduced the idea of processing sequential data but suffers from the **vanishing gradient problem**, which limits learning long-term dependencies.\n",
    "- **LSTM RNN**: Introduced memory cells with gated mechanisms to preserve long-term context and **solve the vanishing gradient problem**.\n",
    "- **GRU RNN**: A simplified version of LSTM that **combines forget and input gates** into an **update gate** for efficiency.\n",
    "- **Bidirectional RNN (BiRNN)**: Processes sequences **both forward and backward**, which helps in tasks where **both past and future context** are important â€” like predicting a **middle word**.\n",
    "\n",
    "> ğŸ§  Example: In the sentence _\"The cat ___ on the mat\"_, predicting the blank requires knowing **both left (\"The cat\") and right (\"on the mat\") context** â€” something BiRNN handles better than standard RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ The Need for Encoder-Decoder Architecture\n",
    "\n",
    "Despite all these advances, thereâ€™s a major limitation with the above models:\n",
    "\n",
    "They assume **input and output sequences are of the same length** and are **processed in a single pass**.\n",
    "\n",
    "> ğŸ” But what if we want to perform **sequence-to-sequence transformations**?\n",
    "\n",
    "### ğŸ¯ Problem Statement\n",
    "\n",
    "> _â€œTranslate the English sentence â€˜I love youâ€™ into French.â€_\n",
    "\n",
    "- Input: \"I love you\" â†’ 3 words\n",
    "- Output: \"Je tâ€™aime\" â†’ 2 words\n",
    "\n",
    "None of the previous models â€” not even LSTM RNNs or BiRNNs â€” can **handle this transformation reliably** because:\n",
    "- The **input and output sequence lengths differ**.\n",
    "- We need to **encode the meaning of the whole input** before generating the output.\n",
    "- We need a way to **generate output one token at a time**, while **remembering the entire input context**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Introducing: Encoder-Decoder Architecture\n",
    "\n",
    "The **Encoder-Decoder** model is designed specifically for tasks like:\n",
    "\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Chatbots\n",
    "- Speech Recognition\n",
    "\n",
    "### ğŸ§© How It Works:\n",
    "\n",
    "- **Encoder**: Reads the input sequence and encodes it into a **fixed-length context vector** (`C`), capturing the entire meaning of the input.\n",
    "- **Decoder**: Takes this context vector and **generates the output sequence** token-by-token.\n",
    "\n",
    "> ğŸ¯ Unlike previous models, Encoder-Decoder handles:\n",
    "> - Input and output sequences of **different lengths**\n",
    "> - Complex **sequence-to-sequence mappings**\n",
    "> - One-to-many or many-to-one transformations\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Transition Summary\n",
    "\n",
    "| Model         | Handles Long-Term Memory | Bidirectional Context | Variable Length Mapping |\n",
    "|---------------|---------------------------|------------------------|---------------------------|\n",
    "| Simple RNN    | âŒ                        | âŒ                     | âŒ                        |\n",
    "| LSTM RNN      | âœ…                        | âŒ                     | âŒ                        |\n",
    "| GRU RNN       | âœ… (Efficient)            | âŒ                     | âŒ                        |\n",
    "| BiRNN         | âœ… (Past + Future)        | âœ…                     | âŒ                        |\n",
    "| **Encoder-Decoder** | âœ… (All above + more) | âœ… (with BiLSTM)       | âœ…                        |\n",
    "\n",
    "Letâ€™s now explore how the Encoder-Decoder architecture works in detail!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a40e0",
   "metadata": {},
   "source": [
    "## ğŸ”„ Encoder-Decoder Architecture (Seq2Seq) â€“ Step-by-Step Breakdown\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ **Goal**\n",
    "The **Encoder-Decoder architecture** is used for sequence-to-sequence (seq2seq) tasks like:\n",
    "\n",
    "- ğŸ—£ï¸ Language translation (e.g., English â†’ French)\n",
    "- ğŸ“ Text summarization\n",
    "- ğŸ§  Chatbots and Q&A systems\n",
    "\n",
    "It **takes one sequence as input** (like a sentence) and **generates another sequence as output** (like the translated sentence).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§± **1. Embedding Layer (First Layer)**\n",
    "- Words (like `\"I\"`, `\"love\"`, `\"you\"`) are **converted into numbers** (word IDs).\n",
    "- These numbers are passed through an **Embedding layer** to get **dense vector representations** of fixed size.\n",
    "\n",
    "ğŸ“Œ **Why?**\n",
    "Because embeddings **capture word meaning** and make learning more effective than one-hot vectors.\n",
    "\n",
    "```python\n",
    "Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¥ **2. Encoder: Sequence Compression**\n",
    "\n",
    "#### ğŸ” Uses RNN variant: usually **LSTM** or **GRU**\n",
    "- **RNNs** struggle with long-term dependencies, so we use **LSTM (Long Short-Term Memory)** or **GRU (Gated Recurrent Unit)**.\n",
    "- These units process input **word-by-word** and **remember relevant information** using:\n",
    "  - ğŸ§  **Short-Term Memory (STM)** â€“ Current word info\n",
    "  - ğŸ§  **Long-Term Memory (LTM)** â€“ What to remember or forget from the past\n",
    "\n",
    "```python\n",
    "encoder_lstm = LSTM(units, return_state=True)\n",
    "```\n",
    "\n",
    "#### ğŸ”¢ Special Tokens\n",
    "- **<SOS> (Start of Sentence)**: Signals the beginning of the sequence.\n",
    "- **<EOS> (End of Sentence)**: Signals the end of the output generation.\n",
    "\n",
    "These tokens help the model know **when to start and stop decoding**.\n",
    "\n",
    "#### ğŸ§  Final Step: **Context Vector**\n",
    "The encoder outputs a **\"context vector\"** (a fixed-size vector) summarizing the entire input sentence.\n",
    "\n",
    "This vector captures:\n",
    "- Grammar\n",
    "- Sentence meaning\n",
    "- Word order\n",
    "- Dependencies\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¤ **3. Decoder: Sequence Generation**\n",
    "\n",
    "The decoder:\n",
    "1. Takes the **context vector** from the encoder.\n",
    "2. Starts with the **<SOS>** token.\n",
    "3. Predicts the next word **one at a time**, using LSTM or GRU.\n",
    "4. Stops when it predicts the **<EOS>** token.\n",
    "\n",
    "At each time step, the decoder uses:\n",
    "- The **previous hidden state**\n",
    "- The **previous predicted word**\n",
    "- The **context vector (initially from the encoder)**\n",
    "\n",
    "```python\n",
    "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š **4. Softmax Layer: Final Prediction**\n",
    "\n",
    "At every decoding step, the output from the decoder LSTM is passed through a **Dense layer with Softmax**:\n",
    "\n",
    "```python\n",
    "Dense(vocab_size, activation='softmax')\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Softmax**\n",
    "- Converts raw scores into **probabilities** for each word in the vocabulary.\n",
    "- The word with the **highest probability** is selected as the output for that step.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” **Why Not Plain RNN?**\n",
    "- **RNNs** can't remember far-back context â†’ leads to poor performance in long sequences.\n",
    "- **LSTM/GRU** solve this by using gates to **remember or forget** info at each step.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”š Summary Workflow\n",
    "\n",
    "1. ğŸ”¡ Input sentence â†’ Embedding\n",
    "2. ğŸ” Embeddings â†’ LSTM/GRU encoder â†’ Final states = **Context Vector**\n",
    "3. ğŸš€ Decoder uses Context + <SOS> to start\n",
    "4. ğŸ”® At each step:\n",
    "   - Predict next word using Softmax\n",
    "   - Feed it back into decoder\n",
    "5. â›” Stop when <EOS> is predicted\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Example (English â†’ French)**\n",
    "\n",
    "Input: `\"I love you\"`  \n",
    "Tokens: `<SOS> I love you <EOS>`  \n",
    "Output: `\"Je tâ€™aime\"` â†’ `<SOS> Je tâ€™ aime <EOS>`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2128a7e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
