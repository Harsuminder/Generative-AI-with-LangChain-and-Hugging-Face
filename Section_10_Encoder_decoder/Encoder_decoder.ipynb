{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1b15fc",
   "metadata": {},
   "source": [
    "## 🔁 Quick Recap: RNNs So Far\n",
    "\n",
    "Before diving into **Encoder-Decoder** models, let’s revisit what we’ve learned:\n",
    "\n",
    "- **Simple RNN**: Introduced the idea of processing sequential data but suffers from the **vanishing gradient problem**, which limits learning long-term dependencies.\n",
    "- **LSTM RNN**: Introduced memory cells with gated mechanisms to preserve long-term context and **solve the vanishing gradient problem**.\n",
    "- **GRU RNN**: A simplified version of LSTM that **combines forget and input gates** into an **update gate** for efficiency.\n",
    "- **Bidirectional RNN (BiRNN)**: Processes sequences **both forward and backward**, which helps in tasks where **both past and future context** are important — like predicting a **middle word**.\n",
    "\n",
    "> 🧠 Example: In the sentence _\"The cat ___ on the mat\"_, predicting the blank requires knowing **both left (\"The cat\") and right (\"on the mat\") context** — something BiRNN handles better than standard RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 The Need for Encoder-Decoder Architecture\n",
    "\n",
    "Despite all these advances, there’s a major limitation with the above models:\n",
    "\n",
    "They assume **input and output sequences are of the same length** and are **processed in a single pass**.\n",
    "\n",
    "> 🔍 But what if we want to perform **sequence-to-sequence transformations**?\n",
    "\n",
    "### 🎯 Problem Statement\n",
    "\n",
    "> _“Translate the English sentence ‘I love you’ into French.”_\n",
    "\n",
    "- Input: \"I love you\" → 3 words\n",
    "- Output: \"Je t’aime\" → 2 words\n",
    "\n",
    "None of the previous models — not even LSTM RNNs or BiRNNs — can **handle this transformation reliably** because:\n",
    "- The **input and output sequence lengths differ**.\n",
    "- We need to **encode the meaning of the whole input** before generating the output.\n",
    "- We need a way to **generate output one token at a time**, while **remembering the entire input context**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Introducing: Encoder-Decoder Architecture\n",
    "\n",
    "The **Encoder-Decoder** model is designed specifically for tasks like:\n",
    "\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Chatbots\n",
    "- Speech Recognition\n",
    "\n",
    "### 🧩 How It Works:\n",
    "\n",
    "- **Encoder**: Reads the input sequence and encodes it into a **fixed-length context vector** (`C`), capturing the entire meaning of the input.\n",
    "- **Decoder**: Takes this context vector and **generates the output sequence** token-by-token.\n",
    "\n",
    "> 🎯 Unlike previous models, Encoder-Decoder handles:\n",
    "> - Input and output sequences of **different lengths**\n",
    "> - Complex **sequence-to-sequence mappings**\n",
    "> - One-to-many or many-to-one transformations\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Transition Summary\n",
    "\n",
    "| Model         | Handles Long-Term Memory | Bidirectional Context | Variable Length Mapping |\n",
    "|---------------|---------------------------|------------------------|---------------------------|\n",
    "| Simple RNN    | ❌                        | ❌                     | ❌                        |\n",
    "| LSTM RNN      | ✅                        | ❌                     | ❌                        |\n",
    "| GRU RNN       | ✅ (Efficient)            | ❌                     | ❌                        |\n",
    "| BiRNN         | ✅ (Past + Future)        | ✅                     | ❌                        |\n",
    "| **Encoder-Decoder** | ✅ (All above + more) | ✅ (with BiLSTM)       | ✅                        |\n",
    "\n",
    "Let’s now explore how the Encoder-Decoder architecture works in detail!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a40e0",
   "metadata": {},
   "source": [
    "## 🔄 Encoder-Decoder Architecture (Seq2Seq) – Step-by-Step Breakdown\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Goal**\n",
    "The **Encoder-Decoder architecture** is used for sequence-to-sequence (seq2seq) tasks like:\n",
    "\n",
    "- 🗣️ Language translation (e.g., English → French)\n",
    "- 📝 Text summarization\n",
    "- 🧠 Chatbots and Q&A systems\n",
    "\n",
    "It **takes one sequence as input** (like a sentence) and **generates another sequence as output** (like the translated sentence).\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 **1. Embedding Layer (First Layer)**\n",
    "- Words (like `\"I\"`, `\"love\"`, `\"you\"`) are **converted into numbers** (word IDs).\n",
    "- These numbers are passed through an **Embedding layer** to get **dense vector representations** of fixed size.\n",
    "\n",
    "📌 **Why?**\n",
    "Because embeddings **capture word meaning** and make learning more effective than one-hot vectors.\n",
    "\n",
    "```python\n",
    "Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📥 **2. Encoder: Sequence Compression**\n",
    "\n",
    "#### 🔁 Uses RNN variant: usually **LSTM** or **GRU**\n",
    "- **RNNs** struggle with long-term dependencies, so we use **LSTM (Long Short-Term Memory)** or **GRU (Gated Recurrent Unit)**.\n",
    "- These units process input **word-by-word** and **remember relevant information** using:\n",
    "  - 🧠 **Short-Term Memory (STM)** – Current word info\n",
    "  - 🧠 **Long-Term Memory (LTM)** – What to remember or forget from the past\n",
    "\n",
    "```python\n",
    "encoder_lstm = LSTM(units, return_state=True)\n",
    "```\n",
    "\n",
    "#### 🔢 Special Tokens\n",
    "- **<SOS> (Start of Sentence)**: Signals the beginning of the sequence.\n",
    "- **<EOS> (End of Sentence)**: Signals the end of the output generation.\n",
    "\n",
    "These tokens help the model know **when to start and stop decoding**.\n",
    "\n",
    "#### 🧠 Final Step: **Context Vector**\n",
    "The encoder outputs a **\"context vector\"** (a fixed-size vector) summarizing the entire input sentence.\n",
    "\n",
    "This vector captures:\n",
    "- Grammar\n",
    "- Sentence meaning\n",
    "- Word order\n",
    "- Dependencies\n",
    "\n",
    "---\n",
    "\n",
    "### 📤 **3. Decoder: Sequence Generation**\n",
    "\n",
    "The decoder:\n",
    "1. Takes the **context vector** from the encoder.\n",
    "2. Starts with the **<SOS>** token.\n",
    "3. Predicts the next word **one at a time**, using LSTM or GRU.\n",
    "4. Stops when it predicts the **<EOS>** token.\n",
    "\n",
    "At each time step, the decoder uses:\n",
    "- The **previous hidden state**\n",
    "- The **previous predicted word**\n",
    "- The **context vector (initially from the encoder)**\n",
    "\n",
    "```python\n",
    "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **4. Softmax Layer: Final Prediction**\n",
    "\n",
    "At every decoding step, the output from the decoder LSTM is passed through a **Dense layer with Softmax**:\n",
    "\n",
    "```python\n",
    "Dense(vocab_size, activation='softmax')\n",
    "```\n",
    "\n",
    "📌 **Softmax**\n",
    "- Converts raw scores into **probabilities** for each word in the vocabulary.\n",
    "- The word with the **highest probability** is selected as the output for that step.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 **Why Not Plain RNN?**\n",
    "- **RNNs** can't remember far-back context → leads to poor performance in long sequences.\n",
    "- **LSTM/GRU** solve this by using gates to **remember or forget** info at each step.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔚 Summary Workflow\n",
    "\n",
    "1. 🔡 Input sentence → Embedding\n",
    "2. 🔁 Embeddings → LSTM/GRU encoder → Final states = **Context Vector**\n",
    "3. 🚀 Decoder uses Context + <SOS> to start\n",
    "4. 🔮 At each step:\n",
    "   - Predict next word using Softmax\n",
    "   - Feed it back into decoder\n",
    "5. ⛔ Stop when <EOS> is predicted\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Example (English → French)**\n",
    "\n",
    "Input: `\"I love you\"`  \n",
    "Tokens: `<SOS> I love you <EOS>`  \n",
    "Output: `\"Je t’aime\"` → `<SOS> Je t’ aime <EOS>`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2128a7e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
