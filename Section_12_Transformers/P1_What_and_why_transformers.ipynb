{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6066007e",
   "metadata": {},
   "source": [
    "## What Are Transformers?\n",
    "\n",
    "Transformers are a type of deep learning model introduced in the paper **\"Attention Is All You Need\" (Vaswani et al., 2017)**. They are designed to process and generate sequences using an **attention mechanism** without relying on recurrence (like RNNs or LSTMs). Transformers are particularly powerful for **Natural Language Processing (NLP)** tasks such as:\n",
    "\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Text Classification\n",
    "- Question Answering\n",
    "- Text Generation\n",
    "\n",
    "Transformers follow an **encoder-decoder architecture**:\n",
    "\n",
    "- The **encoder** processes the input sequence and generates intermediate representations.\n",
    "- The **decoder** uses these representations along with previously generated outputs to produce the target sequence.\n",
    "\n",
    "This is especially useful for **sequence-to-sequence** tasks, such as translating a sentence from English to French (e.g., Google Translate).\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use Transformers?\n",
    "\n",
    "### 1. Overcoming the Limitations of Traditional Encoder-Decoder Models\n",
    "\n",
    "Traditional models like LSTM or GRU compress an entire input sentence into a **single fixed-length context vector**. This becomes problematic as the input sentence gets longer:\n",
    "\n",
    "- **Earlier words** get less attention.\n",
    "- Performance degrades on long sentences.\n",
    "- Metrics like **BLEU score** drop with increasing length.\n",
    "\n",
    "To solve this, **attention mechanisms** were introduced, allowing the decoder to look at **all input words** dynamically, giving additional context at each decoding step. This significantly improves performance on long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Parallelization and Scalability\n",
    "\n",
    "A major limitation of RNN-based models is that they process input **sequentially**—word by word:\n",
    "\n",
    "- This makes training **slow** and **non-parallelizable**.\n",
    "- Not scalable for large datasets.\n",
    "\n",
    "Transformers solve this by using **self-attention**:\n",
    "\n",
    "- They process all words **in parallel**.\n",
    "- **Positional encoding** is added to preserve word order.\n",
    "- This makes transformers much more scalable and efficient to train.\n",
    "\n",
    "Because of this, transformers are the backbone of many **state-of-the-art (SOTA)** models in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Contextual Embeddings\n",
    "\n",
    "Earlier models used **static word embeddings**, meaning:\n",
    "\n",
    "> The word \"I\" always had the same vector, regardless of its meaning in different contexts.\n",
    "\n",
    "Transformers use **contextual embeddings**:\n",
    "\n",
    "- The embedding of a word depends on the **entire sentence**.\n",
    "- For example, in the sentence:  \n",
    "  _\"My name is Harry and I want to play cricket.\"_  \n",
    "  The word \"I\" is understood in the context of \"Harry\".\n",
    "\n",
    "This leads to much better understanding of **word meaning and relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Transfer Learning and Multimodal Tasks\n",
    "\n",
    "Transformers also enable powerful **transfer learning**:\n",
    "\n",
    "- Large models (like BERT, GPT, T5) can be **pre-trained** on vast datasets and **fine-tuned** on smaller, specific tasks.\n",
    "- This has made transformers highly reusable across many NLP problems.\n",
    "\n",
    "In addition, they have been successfully used in **multimodal tasks** (e.g., combining text and images), such as:\n",
    "\n",
    "- Image Captioning\n",
    "- Visual Question Answering\n",
    "- Text-to-Image Generation\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "\n",
    "Transformers solve key problems in older NLP models by:\n",
    "\n",
    "- Using attention instead of recurrence\n",
    "- Allowing for parallel training\n",
    "- Generating context-aware embeddings\n",
    "- Scaling well with large data and tasks\n",
    "\n",
    "They are now the **foundation of modern NLP**, powering models like BERT, GPT, and many others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4a302",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
