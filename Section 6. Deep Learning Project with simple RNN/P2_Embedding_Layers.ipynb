{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd077f2",
   "metadata": {},
   "source": [
    "## üî¢ Getting started with Embedding Layer\n",
    "\n",
    "Before feeding text into machine learning models, we must first **convert words into numerical representations**. One of the simplest approaches is **One-Hot Encoding**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© One-Hot Encoding\n",
    "\n",
    "In One-Hot Encoding, each word is assigned a unique index and represented by a binary vector of length equal to the vocabulary size. Only one position is `1`, and all others are `0`.\n",
    "\n",
    "Example:  \n",
    "If `\"movie\"` is index `345` in a vocabulary of size `10,000`, its one-hot vector looks like:  \n",
    "`[0, 0, ..., 1 (at 345), ..., 0]`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è The Sparsity Problem\n",
    "\n",
    "One-hot vectors come with key limitations:\n",
    "\n",
    "1. **High Dimensionality**: A 10,000-word vocabulary leads to 10,000-dimensional vectors.\n",
    "2. **Sparsity**: Most values are zero, which is inefficient for memory and computation.\n",
    "3. **No Semantics**: All vectors are orthogonal, so `\"great\"` and `\"terrible\"` are equally dissimilar as `\"great\"` and `\"banana\"` ‚Äî there's no notion of meaning or similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Feature Representation with Embeddings\n",
    "\n",
    "To overcome this, we use **Word Embeddings** ‚Äî compact, dense vectors that capture **semantic similarity**. These vectors represent **features** of words such as sentiment, gender, or domain usage. Unlike one-hot vectors, embeddings learned during training bring semantically related words closer together in vector space.\n",
    "\n",
    "Example:\n",
    "- `\"great\"` ‚Üí `[0.12, -0.45, 0.67, ..., 0.01]`\n",
    "- `\"good\"` ‚Üí `[0.14, -0.40, 0.65, ..., 0.03]`\n",
    "- `\"bad\"` ‚Üí `[-0.55, 0.72, -0.23, ..., -0.01]`\n",
    "\n",
    "These embeddings form the **feature representation** of words and are used as input to models like RNNs and LSTMs. The model learns to associate certain **dimensions of the embedding** with task-relevant characteristics (e.g., positivity, royalty, action, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Word2Vec: Learning Embeddings Based on Context\n",
    "\n",
    "**Word2Vec** is a widely used algorithm that learns word embeddings from large corpora using two main architectures:\n",
    "\n",
    "- **CBOW (Continuous Bag of Words)**: Predicts a word from surrounding context words.\n",
    "- **Skip-Gram**: Predicts surrounding words from a target word.\n",
    "\n",
    "Word2Vec embeddings capture semantic relationships and even analogies:  \n",
    "`vector(\"king\") - vector(\"man\") + vector(\"woman\") ‚âà vector(\"queen\")`\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Embedding Layer in Deep Learning\n",
    "\n",
    "Deep learning frameworks (e.g., TensorFlow, Keras) provide an **Embedding layer** to learn word representations as part of the training process. It functions like a **trainable lookup table** that maps each word index to a dense vector.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db298a80",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
