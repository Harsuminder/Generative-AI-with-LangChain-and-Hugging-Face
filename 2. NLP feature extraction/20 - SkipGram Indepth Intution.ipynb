{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Word2Vec ‚Äì Skip-Gram Model (In-Depth Intuition)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå What is Skip-Gram?\n",
    "\n",
    "The **Skip-Gram** model is the second architecture introduced in the original **Word2Vec** paper by Mikolov et al. Unlike CBOW, which predicts the **center word from its context**, **Skip-Gram does the opposite**:\n",
    "\n",
    "> üéØ **It predicts the context words given the center word.**\n",
    "\n",
    "This makes it powerful for learning high-quality embeddings, especially for **rare or infrequent words**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How Does It Work?\n",
    "\n",
    "Given a sentence like:  \n",
    "`The quick brown fox jumps over the lazy dog`\n",
    "\n",
    "Let‚Äôs assume:\n",
    "- **Window size = 2**\n",
    "- **Target word = \"brown\"**\n",
    "\n",
    "Then Skip-Gram will try to predict the surrounding context:\n",
    "- `(\"brown\", \"the\")`\n",
    "- `(\"brown\", \"quick\")`\n",
    "- `(\"brown\", \"fox\")`\n",
    "- `(\"brown\", \"jumps\")`\n",
    "\n",
    "So we generate **multiple (input, output) pairs**, all centered around the same target word.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Intuition Behind Skip-Gram\n",
    "\n",
    "Skip-Gram turns one word into **many training examples**. For each word in the corpus, it creates a number of (center ‚Üí context) predictions depending on the window size.\n",
    "\n",
    "- **Input**: One center word (e.g., `\"brown\"`)\n",
    "- **Output**: One of its surrounding context words (e.g., `\"fox\"`)\n",
    "\n",
    "The model learns to associate the center word with the types of words that tend to appear near it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Skip-Gram: Training Objective and Loss Function \n",
    "\n",
    "The **Skip-Gram** model learns word embeddings by doing one simple thing:\n",
    "\n",
    "> Given a word (the **center word**), try to **predict the words around it** (the **context words**).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Example:\n",
    "\n",
    "**Sentence:**  \n",
    "`The quick brown fox jumps`\n",
    "\n",
    "Let‚Äôs say the center word is `\"brown\"` and we use a window size of 2.  \n",
    "Then we‚Äôll try to predict the following pairs:\n",
    "\n",
    "- `\"brown\"` ‚Üí `\"the\"`\n",
    "- `\"brown\"` ‚Üí `\"quick\"`\n",
    "- `\"brown\"` ‚Üí `\"fox\"`\n",
    "- `\"brown\"` ‚Üí `\"jumps\"`\n",
    "\n",
    "So for each center word, the model generates multiple training examples ‚Äî one for each nearby word.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What Does the Model Learn?\n",
    "\n",
    "The model starts with **random word vectors**. During training, it adjusts these vectors so that words that appear in similar contexts end up **close together** in the vector space.\n",
    "\n",
    "For example:\n",
    "- `\"king\"` and `\"queen\"` appear in similar contexts like `\"royal\"`, `\"palace\"`, `\"throne\"` ‚Üí they end up with similar vectors\n",
    "- `\"apple\"` and `\"banana\"` may appear in fruit-related sentences ‚Üí also close together\n",
    "\n",
    "---\n",
    "\n",
    "## üí° The Goal (Loss Function)\n",
    "\n",
    "The goal is to make the model:\n",
    "- **Increase the score** when the context word is correct (like `\"brown\"` ‚Üí `\"fox\"`)\n",
    "- **Decrease the score** when the context word is wrong (like `\"brown\"` ‚Üí `\"car\"`)\n",
    "\n",
    "The model repeats this process for **millions of word pairs**, slowly improving the word vectors so they reflect real-world meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Why is Training Hard?\n",
    "\n",
    "To predict context words, the model technically needs to compare the center word to **every word in the vocabulary** ‚Äî which could be millions of words. That‚Äôs too slow.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° The Solution: Negative Sampling\n",
    "\n",
    "To fix this, Word2Vec uses **negative sampling**:\n",
    "- Instead of checking every possible word, the model only checks a **small number of incorrect words** (called \"negative samples\")\n",
    "- For example: it learns `\"brown\"` ‚Üí `\"fox\"` is correct, but also learns `\"brown\"` ‚Üí `\"car\"` or `\"window\"` are wrong\n",
    "\n",
    "This makes training **much faster** and still very effective.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- **Skip-Gram** learns by predicting nearby words from a given word\n",
    "- It creates many (center ‚Üí context) word pairs during training\n",
    "- The model improves word vectors so that related words are closer together\n",
    "- **Negative sampling** makes this training process fast and scalable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Why Use Skip-Gram?\n",
    "\n",
    "- Works well on **small datasets**\n",
    "- **Better at capturing semantic relationships for rare words**\n",
    "- Slower than CBOW but often more **accurate**\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Key Insight\n",
    "\n",
    "If two words appear in **similar contexts**, they will learn **similar embeddings**.\n",
    "\n",
    "For example:\n",
    "- `\"king\"` and `\"queen\"` might both appear near `\"royal\"`, `\"crown\"`, or `\"kingdom\"`\n",
    "- This results in embeddings that are close in vector space\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Summary\n",
    "\n",
    "| Feature              | CBOW                                      | Skip-Gram                                  |\n",
    "|----------------------|-------------------------------------------|---------------------------------------------|\n",
    "| Input                | Context words                             | Target (center) word                        |\n",
    "| Output               | Center word                               | One context word at a time                  |\n",
    "| Training Speed       | Faster                                    | Slightly slower                             |\n",
    "| Best For             | Frequent words, large datasets            | Rare/infrequent words, smaller datasets     |\n",
    "| Embedding Quality    | Good for general meaning                  | Better for capturing fine-grained meaning   |\n",
    "| Accuracy (Rare Words)| Lower                                     | Higher                                      |\n",
    "| Memory Usage         | Lower                                     | Slightly higher                             |\n",
    "| Example Pair(s)      | `[\"the\", \"quick\"] ‚Üí \"brown\"`              | `\"brown\" ‚Üí \"quick\"`, `\"brown\" ‚Üí \"fox\"`      |\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Tip:** Use **CBOW** for faster training with large corpora, and **Skip-Gram** for better results on rare words or smaller datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ How to Improve CBOW or Skip-Gram Word2Vec Models\n",
    "\n",
    "Whether you're using **CBOW** or **Skip-Gram**, there are several ways to improve the quality of your word embeddings. Here are some effective strategies:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. üßπ Clean and Preprocess Your Text\n",
    "\n",
    "- Remove stopwords, punctuation, and noisy symbols\n",
    "- Lowercase all words for consistency\n",
    "- Lemmatize or stem words to reduce sparsity\n",
    "- Filter out extremely short or irrelevant sentences\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üìè Tune Key Hyperparameters\n",
    "\n",
    "- **`vector_size`**: Higher dimensions capture more detail. Try 100, 200, or 300 (Google uses 300).\n",
    "- **`window`**: Try larger windows (e.g., 5 or 10) to capture broader context, especially in semantic-heavy tasks.\n",
    "- **`min_count`**: Set to 2‚Äì5 to filter out rare/noisy words.\n",
    "- **`epochs`**: Increase the number of training iterations (e.g., 10‚Äì30) for better convergence.\n",
    "- **`negative`**: Tune the number of negative samples (5‚Äì20 is typical for Skip-Gram).\n",
    "- **`sg`**: 0 for CBOW, 1 for Skip-Gram ‚Äî test both if unsure which fits best.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üìà Use More (and Better) Data\n",
    "\n",
    "- Train on a **larger corpus** to improve word coverage and embedding quality.\n",
    "- Domain-specific corpora yield **more relevant embeddings** for specialized tasks (e.g., legal, finance, medicine).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "Improving Word2Vec models (CBOW or Skip-Gram) is a mix of:\n",
    "- **Better data**\n",
    "- **Smarter preprocessing**\n",
    "- **Hyperparameter tuning**\n",
    "\n",
    "Always experiment to find the best setup for your specific dataset and goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
