{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are N-Grams?\n",
    "\n",
    "**N-Grams** are continuous sequences of *n* items (usually words or characters) extracted from a text. In NLP, they are used to preserve word order and capture local context.\n",
    "\n",
    "- **Unigram (n = 1)** ‚Üí Single words: `[\"NLP\", \"is\", \"fun\"]`\n",
    "- **Bigram (n = 2)** ‚Üí Pairs of consecutive words: `[(\"NLP\", \"is\"), (\"is\", \"fun\")]`\n",
    "- **Trigram (n = 3)** ‚Üí Triplets: `[(\"NLP\", \"is\", \"fun\")]`\n",
    "\n",
    "N-Grams are useful in many NLP tasks such as language modeling, text classification, and sentiment analysis, where the order of words can significantly affect meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Why Not Just Use Bag of Words?\n",
    "\n",
    "While **Bag of Words (BoW)** captures word frequency, it completely ignores the **order** of words. This leads to major issues in understanding meaning.\n",
    "\n",
    "#### Consider:\n",
    "\n",
    "- **Sentence 1**: `\"Food is good\"`  \n",
    "- **Sentence 2**: `\"Food is not good\"`\n",
    "\n",
    "Despite opposite meanings, BoW treats both as very similar because they share the same set of words.\n",
    "\n",
    "#### Their BoW vectors might look like this:\n",
    "\n",
    "| Word     | food | is | good | not |\n",
    "|----------|------|----|------|-----|\n",
    "| Sentence 1 |  1   | 1  |  1   |  0  |\n",
    "| Sentence 2 |  1   | 1  |  1   |  1  |\n",
    "\n",
    "As you can see, both vectors are almost identical, however, sentiment is completely opposite.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† N-Grams in Action: \"Food is good\" vs \"Food is not good\"\n",
    "\n",
    "Let‚Äôs see how N-Grams help capture **meaning through word combinations**, which Bag of Words misses.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Input Sentences:\n",
    "\n",
    "1. `\"Food is good\"`  \n",
    "2. `\"Food is not good\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Why N-Grams Help\n",
    "\n",
    "By including bigrams such as:\n",
    "- (\"is\", \"good\")\n",
    "- (\"not\", \"good\")\n",
    "\n",
    "N-Grams allow us to represent these phrases explicitly. This way, the model can distinguish between **\"good\"** and **\"not good\"**, even though both words appear in isolation.\n",
    "\n",
    "This makes N-Grams especially powerful in tasks like **sentiment analysis**, **intent detection**, and **contextual understanding**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
