{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß∫ Bag of Words (BoW)\n",
    "\n",
    "**Bag of Words (BoW)** is a natural extension of **one-hot encoding** ‚Äî instead of just representing the **presence or absence** of words, BoW also captures **how frequently** each word appears in a document.\n",
    "\n",
    "Like one-hot encoding, it converts text into numerical vectors. But while one-hot represents each word individually, BoW treats the entire sentence or document as a single unit and builds a **frequency-based feature vector** using the words in a fixed vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° What Problems Can BoW Help Solve?\n",
    "\n",
    "BoW can be effectively used in:\n",
    "- **Text classification** (e.g., spam detection, sentiment analysis)\n",
    "- **Document similarity** or search ranking\n",
    "- As input for traditional ML models like Naive Bayes, SVMs, or Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ Step-by-Step Process:\n",
    "\n",
    "Let‚Äôs say we have 3 sentences:\n",
    "1. `\"I love NLP\"`  \n",
    "2. `\"NLP is fun\"`  \n",
    "3. `\"I love fun\"`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 1: Lowercase All Words\n",
    "Convert all text to lowercase for consistency:\n",
    "- `\"i love nlp\"`\n",
    "- `\"nlp is fun\"`\n",
    "- `\"i love fun\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ Step 2: Remove Stop Words\n",
    "Stop words (like *is*, *the*, *and*) don‚Äôt carry much meaning. We remove them:\n",
    "- `\"i love nlp\"` ‚Üí `\"love nlp\"`\n",
    "- `\"nlp is fun\"` ‚Üí `\"nlp fun\"`\n",
    "- `\"i love fun\"` ‚Üí `\"love fun\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üî§ Step 3: Extract Unique Words\n",
    "From all sentences, extract unique non-stop words (the vocabulary):\n",
    "\n",
    "[\"love\", \"nlp\", \"fun\"]\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Step 4: Vocabulary with Frequencies (Descending Order)\n",
    "Count how often each word appears across all documents:\n",
    "- `\"love\"` ‚Üí 3\n",
    "- `\"nlp\"` ‚Üí 2\n",
    "- `\"fun\"` ‚Üí 2\n",
    "\n",
    "Sort by frequency:\n",
    "[\"love\", \"nlp\", \"fun\"]\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÇÔ∏è Step 5: Use Top Features Only (Optional)\n",
    "In real-world datasets with large vocabularies, we often select the **top N most frequent words** (e.g., top 10) and ignore the rest. This reduces dimensionality and noise.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Step 6: Create Document-Term Matrix\n",
    "\n",
    "Now we build a matrix with:\n",
    "- Rows = sentences/documents\n",
    "- Columns = vocabulary words (features)\n",
    "- Values = word frequency per sentence\n",
    "\n",
    "| Sentence       | love | nlp | fun |\n",
    "|----------------|------|-----|-----|\n",
    "| \"love nlp\"     | 1    | 1   | 0   |\n",
    "| \"nlp fun\"      | 0    | 1   | 1   |\n",
    "| \"love fun\"     | 1    | 0   | 1   |\n",
    "\n",
    "---\n",
    "\n",
    "- **Frequency BoW**: Count how many times each word appears.\n",
    "- **Binary BoW**: Use `1` if the word appears in the sentence, `0` if it doesn't ‚Äî ignoring how many times it appears.\n",
    "\n",
    "---\n",
    "\n",
    "This simple but powerful technique allows us to feed raw text into classic machine learning models by converting it into a structured, numerical format."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
