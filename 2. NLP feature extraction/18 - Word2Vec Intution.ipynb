{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Word2Vec Intuition\n",
    "\n",
    "**Word2Vec** is a word embedding algorithm developed by **Google in 2013** that represents words as **dense, continuous vectors** based on their context in large text corpora.\n",
    "\n",
    "Rather than relying on word frequency (like BoW or TF-IDF), Word2Vec learns **semantic meaning** by training a shallow neural network to understand how words relate to one another based on their surrounding words.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Key Concepts\n",
    "\n",
    "- Each word is mapped to a **vector of real numbers** (e.g., 100 or 300 dimensions).\n",
    "- Words that occur in **similar contexts** are placed **closer together** in the vector space.\n",
    "- Word2Vec captures **semantic similarity** (e.g., `\"France\"` and `\"Italy\"` are close) and **linguistic relationships** (e.g., `\"king - man + woman ‚âà queen\"`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Vocabulary and Feature Space in Word2Vec\n",
    "\n",
    "In Word2Vec, every **unique word** in the corpus becomes part of the **vocabulary**. Each word is associated with a **trainable vector** of fixed length ‚Äî typically 100 to 300 dimensions.\n",
    "\n",
    "Unlike sparse representations (like One-Hot Encoding), Word2Vec learns **dense vectors** that capture **semantic meaning and relationships** between words.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Vocabulary ‚Üí Feature Matrix\n",
    "\n",
    "Let‚Äôs say we have the following 5 words in our vocabulary:\n",
    "\n",
    "**Vocabulary:** `[\"king\", \"queen\", \"man\", \"woman\", \"throne\"]`  \n",
    "We assume each word is mapped to a **4-dimensional vector** for simplicity as:\n",
    "\n",
    "- **Power**  \n",
    "- **Royalty**  \n",
    "- **Gender (Masculine/Feminine)**  \n",
    "- **Status**\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Word2Vec Embedding Matrix (Example)\n",
    "\n",
    "| Word     | Power | Royalty | Gender | Status |\n",
    "|----------|--------|---------|--------|--------|\n",
    "| king     | 0.80   | 0.95    | 0.20   | 0.90   |\n",
    "| queen    | 0.78   | 0.96    | 0.80   | 0.88   |\n",
    "| man      | 0.75   | 0.10    | 0.15   | 0.60   |\n",
    "| woman    | 0.72   | 0.12    | 0.85   | 0.62   |\n",
    "| throne   | 0.85   | 0.99    | 0.50   | 0.95   |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Interpretation\n",
    "\n",
    "- `\"king\"` and `\"queen\"` are both **high in power and royalty**, but differ in **gender**.\n",
    "- `\"man\"` and `\"woman\"` are not associated with royalty but are high in **gender polarity**.\n",
    "- `\"throne\"` scores high in **royalty** and **status**, connecting it to both `\"king\"` and `\"queen\"`.\n",
    "\n",
    "> Word2Vec learns these embeddings from **context**, not from predefined labels ‚Äî but similar relationships often emerge naturally during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÅ Vector Relationships (Analogies)\n",
    "\n",
    "One of the most powerful aspects of Word2Vec is its ability to capture **word analogies** using simple vector arithmetic.\n",
    "\n",
    "For example:\n",
    "``king - man + woman ‚âà queen``\n",
    "\n",
    "How this works:\n",
    "\n",
    "- `\"king\"` and `\"man\"` share similar masculine traits, so subtracting `\"man\"` removes the **male component**.\n",
    "- Adding `\"woman\"` brings in the **female context**.\n",
    "- The result vector is close to `\"queen\"` ‚Äî which shares royalty and status with `\"king\"`, but is more feminine.\n",
    "\n",
    "‚úÖ This analogy shows how **semantic relationships** are preserved in the embedding space.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó How Word Similarity is Measured in Word2Vec\n",
    "\n",
    "Once we have word vectors from Word2Vec, we can measure how similar two words are using **Cosine Similarity**.\n",
    "\n",
    "### üìê What is Cosine Similarity?\n",
    "\n",
    "Cosine similarity calculates the **angle between two vectors** in the embedding space ‚Äî not their magnitude.  \n",
    "It ranges from **-1 to 1**:\n",
    "- `1` ‚Üí perfectly similar (same direction)\n",
    "- `0` ‚Üí no similarity (orthogonal)\n",
    "- `-1` ‚Üí opposite direction (completely dissimilar)\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Cosine Similarity Formula\n",
    "\n",
    "For two word vectors **A** and **B**:\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\| \\times \\|B\\|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( A . B \\) is the **dot product** of the vectors\n",
    "- \\( \\|A\\| \\) and \\( \\|B\\| \\) are the **magnitudes** (Euclidean norms) of each vector\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let‚Äôs say we have these simplified 2D word vectors:\n",
    "\n",
    "- **king** = [0.8, 0.5]  \n",
    "- **queen** = [0.75, 0.6]  \n",
    "- **apple** = [-0.2, 0.9]  \n",
    "\n",
    "We can visualize them in 2D space like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           ‚Üë\n",
    "           |                   * queen\n",
    "           |                  ‚Üó\n",
    "           |                 /\n",
    "       1.0 |                /\n",
    "           |               /\n",
    "           |              /\n",
    "           |             /\n",
    "           |            /\n",
    "       0.5 |      * king\n",
    "           |         ‚Üó\n",
    "           |        /\n",
    "           |       /\n",
    "           |      /\n",
    "       0.0 |-----*--------------------‚Üí\n",
    "           |   origin           * apple\n",
    "           |                      ‚Üñ\n",
    "           |                       \\\n",
    "           |                        \\\n",
    "       -0.5|                         \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **king** and **queen** have vectors pointing in similar directions ‚Üí **high cosine similarity**\n",
    "- **apple** points in a different direction ‚Üí **low similarity** with **king** or **queen**\n",
    "\n",
    "### Summary\n",
    "\n",
    "Cosine similarity focuses on the **direction** of vectors, not their length. So, two words can be far apart numerically but still have similar meanings if they point in similar directions in vector space.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
