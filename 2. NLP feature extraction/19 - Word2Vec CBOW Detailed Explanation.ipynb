{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Word2Vec ‚Äì CBOW Model \n",
    "\n",
    "---\n",
    "\n",
    "## üìå What is Word2Vec?\n",
    "\n",
    "**Word2Vec** is a popular technique in Natural Language Processing (NLP) for learning **vector representations of words**, known as **word embeddings**.\n",
    "\n",
    "These embeddings capture semantic meaning‚Äîwords that appear in similar contexts have similar vector representations. For example, the vectors for `\"king\"` and `\"queen\"` would be close in the embedding space.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Word2Vec Architectures\n",
    "\n",
    "Word2Vec has two main architectures:\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)**  \n",
    "   - Predicts the **target (center) word** from a given **context (surrounding words)**  \n",
    "   - Efficient and works well with smaller datasets  \n",
    "   - Emphasizes frequent words\n",
    "\n",
    "2. **Skip-Gram**  \n",
    "   - Predicts **context words** given a **target word**  \n",
    "   - Performs better with rare words  \n",
    "   - More expressive but slightly slower\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Word2Vec: Pretrained vs. Train From Scratch\n",
    "\n",
    "Word2Vec can be used in two primary ways:\n",
    "\n",
    "1. **Pretrained Word2Vec Models**  \n",
    "   - Trained on massive corpora like Google News (100 billion words)  \n",
    "   - Ready-to-use embeddings for general NLP tasks  \n",
    "   - Example: Google‚Äôs Word2Vec (`GoogleNews-vectors-negative300.bin`)\n",
    "\n",
    "2. **Training From Scratch**  \n",
    "   - Ideal for domain-specific tasks or custom vocabulary  \n",
    "   - Gives you full control over the corpus and training process  \n",
    "   - Libraries like Gensim make this easy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç How Does CBOW Work? ‚Äì Step-by-Step Explanation\n",
    "\n",
    "The **Continuous Bag of Words (CBOW)** model predicts the **target (center) word** using its **context (surrounding words)**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Let‚Äôs Take a Simple Example:\n",
    "\n",
    "**Sentence:**  \n",
    "`The quick brown fox jumps over the lazy dog`\n",
    "\n",
    "Let‚Äôs say we set the `window size = 2` ‚Äî this means we‚Äôll look at **2 words before and after** the center word.\n",
    "\n",
    "Now, suppose our **target (center) word** is:  \n",
    "‚û°Ô∏è `\"brown\"`\n",
    "\n",
    "Then the **context words** are:  \n",
    "‚û°Ô∏è `[\"the\", \"quick\", \"fox\", \"jumps\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How CBOW Processes This:\n",
    "\n",
    "1. **Input Layer**:  \n",
    "   - Each context word is converted to a one-hot vector (or embedding)\n",
    "   - So we have 4 input vectors: one for each context word\n",
    "\n",
    "2. **Hidden Layer**:  \n",
    "   - The vectors are averaged (or summed) together\n",
    "   - This gives a single vector representing the combined context\n",
    "\n",
    "3. **Output Layer**:  \n",
    "   - The context vector is multiplied by a weight matrix and passed through a softmax function\n",
    "   - This outputs a probability distribution over all words in the vocabulary\n",
    "\n",
    "4. **Prediction**:  \n",
    "   - The model tries to **maximize the probability of the actual center word** (\"brown\" in our case)\n",
    "   - During training, weights are updated to improve predictions\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ This Process Repeats Across the Corpus\n",
    "\n",
    "CBOW slides a window across the text and learns embeddings by predicting the center word each time. Over many iterations, the model **learns word meanings based on their context**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Goal of CBOW**:  \n",
    "Words appearing in **similar contexts** should have **similar embeddings** ‚Äî this is how semantic relationships are learned!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Feature Representation in CBOW ‚Äì Role of Window Size\n",
    "\n",
    "In the **CBOW (Continuous Bag of Words)** model, the input to the model is a set of **context words**, and the output is the **target (center) word**. The way we construct the input depends directly on the **window size**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Example Sentence:\n",
    "\n",
    "`The quick brown fox jumps`\n",
    "\n",
    "Let‚Äôs say the **target word** is `\"brown\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Case 1: `window = 1`\n",
    "\n",
    "Context words: `[\"quick\", \"fox\"]`  \n",
    "- Each word is converted to a one-hot vector (or an embedding vector)\n",
    "- These vectors are **averaged or summed** to form a single **input vector**\n",
    "- This vector is used to predict `\"brown\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Case 2: `window = 2`\n",
    "\n",
    "Context words: `[\"the\", \"quick\", \"fox\", \"jumps\"]`  \n",
    "- Now we have **4 input vectors**\n",
    "- These are again combined (typically averaged) to produce the final feature vector representing the context\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Vector Size (Embedding Dimension)\n",
    "\n",
    "Each word is represented as a **dense vector of fixed size**.  \n",
    "- The **dimension of this vector** is a hyperparameter called `vector_size` or `embedding_size`\n",
    "- In the original **Google Word2Vec** model, they used:  \n",
    "  üëâ **`vector_size = 300`**\n",
    "\n",
    "This means:\n",
    "- Each word is mapped to a **300-dimensional vector**\n",
    "- If the context window has 4 words, the input consists of **4 such vectors**\n",
    "- These are averaged to form a **single 300-dimensional input vector** for the CBOW model\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What Changes with Window Size?\n",
    "\n",
    "- **Larger window ‚Üí more context words ‚Üí more input vectors to average**\n",
    "- All vectors still have the same dimensionality (`vector_size`)\n",
    "- A larger window provides a **richer and broader context**, potentially improving semantic understanding\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary\n",
    "\n",
    "- CBOW predicts the center word using surrounding context\n",
    "- **Window size** controls how many context words are used\n",
    "- Each context word is represented by a vector of size `vector_size` (e.g., 300 in Google Word2Vec)\n",
    "- These vectors are averaged to create the input feature for prediction\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
