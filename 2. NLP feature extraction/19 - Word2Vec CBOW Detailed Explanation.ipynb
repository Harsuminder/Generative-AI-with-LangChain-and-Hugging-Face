{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Word2Vec ‚Äì CBOW Model \n",
    "\n",
    "---\n",
    "\n",
    "## üìå What is Word2Vec?\n",
    "\n",
    "**Word2Vec** is a popular technique in Natural Language Processing (NLP) for learning **vector representations of words**, known as **word embeddings**.\n",
    "\n",
    "These embeddings capture semantic meaning‚Äîwords that appear in similar contexts have similar vector representations. For example, the vectors for `\"king\"` and `\"queen\"` would be close in the embedding space.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Word2Vec Architectures\n",
    "\n",
    "Word2Vec has two main architectures:\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)**  \n",
    "   - Predicts the **target (center) word** from a given **context (surrounding words)**  \n",
    "   - Efficient and works well with smaller datasets  \n",
    "   - Emphasizes frequent words\n",
    "\n",
    "2. **Skip-Gram**  \n",
    "   - Predicts **context words** given a **target word**  \n",
    "   - Performs better with rare words  \n",
    "   - More expressive but slightly slower\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Word2Vec: Pretrained vs. Train From Scratch\n",
    "\n",
    "Word2Vec can be used in two primary ways:\n",
    "\n",
    "1. **Pretrained Word2Vec Models**  \n",
    "   - Trained on massive corpora like Google News (100 billion words)  \n",
    "   - Ready-to-use embeddings for general NLP tasks  \n",
    "   - Example: Google‚Äôs Word2Vec (`GoogleNews-vectors-negative300.bin`)\n",
    "\n",
    "2. **Training From Scratch**  \n",
    "   - Ideal for domain-specific tasks or custom vocabulary  \n",
    "   - Gives you full control over the corpus and training process  \n",
    "   - Libraries like Gensim make this easy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç How Does CBOW Work? ‚Äì Step-by-Step Explanation\n",
    "\n",
    "The **Continuous Bag of Words (CBOW)** model predicts the **target (center) word** using its **context (surrounding words)**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Let‚Äôs Take a Simple Example:\n",
    "\n",
    "**Sentence:**  \n",
    "`The quick brown fox jumps over the lazy dog`\n",
    "\n",
    "Let‚Äôs say we set the `window size = 2` ‚Äî this means we‚Äôll look at **2 words before and after** the center word.\n",
    "\n",
    "Now, suppose our **target (center) word** is:  \n",
    "‚û°Ô∏è `\"brown\"`\n",
    "\n",
    "Then the **context words** are:  \n",
    "‚û°Ô∏è `[\"the\", \"quick\", \"fox\", \"jumps\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How CBOW Processes This:\n",
    "\n",
    "1. **Input Layer**:  \n",
    "   - Each context word is converted to a one-hot vector (or embedding)\n",
    "   - So we have 4 input vectors: one for each context word\n",
    "\n",
    "2. **Hidden Layer**:  \n",
    "   - The vectors are averaged (or summed) together\n",
    "   - This gives a single vector representing the combined context\n",
    "\n",
    "3. **Output Layer**:  \n",
    "   - The context vector is multiplied by a weight matrix and passed through a softmax function\n",
    "   - This outputs a probability distribution over all words in the vocabulary\n",
    "\n",
    "4. **Prediction**:  \n",
    "   - The model tries to **maximize the probability of the actual center word** (\"brown\" in our case)\n",
    "   - During training, weights are updated to improve predictions\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ This Process Repeats Across the Corpus\n",
    "\n",
    "CBOW slides a window across the text and learns embeddings by predicting the center word each time. Over many iterations, the model **learns word meanings based on their context**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Goal of CBOW**:  \n",
    "Words appearing in **similar contexts** should have **similar embeddings** ‚Äî this is how semantic relationships are learned!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
