{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“˜ TF-IDF and N-grams: Feature Extraction for Text\n",
    "\n",
    "This notebook demonstrates how to convert a text corpus into numerical features using **TF-IDF (Term Frequencyâ€“Inverse Document Frequency)** and **N-Grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library for data manipulation and analysis, especially for working with DataFrames\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file 'spamclassification.csv' from the 'Datasets' folder using pandas\n",
    "# Specifying encoding='latin1' to avoid UnicodeDecodeError with special characters\n",
    "messages = pd.read_csv('Datasets/spamclassification.csv', encoding='latin1')\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to get a quick look at the data\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expression module for basic text cleaning\n",
    "import re\n",
    "\n",
    "# Import the Natural Language Toolkit (nltk) library for text processing\n",
    "import nltk\n",
    "\n",
    "# Import a list of common English stopwords (e.g., \"and\", \"is\", \"with\") from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import the WordNet Lemmatizer from NLTK for reducing words to their base/dictionary form\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazy available bugis n great world la e buffet cine got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry wkly comp win fa cup final tkts st may text fa receive entry question std txt rate c apply',\n",
       " 'u dun say early hor u c already say',\n",
       " 'nah think go usf life around though']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []  # Initialize an empty list to store the cleaned and processed text data\n",
    "\n",
    "for i in range(0, len(messages)):\n",
    "    # Remove all characters except alphabets from the message text\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['text'][i])\n",
    "\n",
    "    # Convert all characters to lowercase\n",
    "    review = review.lower()\n",
    "\n",
    "    # Split the sentence into individual words (tokenization)\n",
    "    review = review.split()\n",
    "\n",
    "    # Apply stemming to each word and remove stopwords \n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words('english')]\n",
    "\n",
    "    # Join the processed words back into a single string\n",
    "    review = ' '.join(review)\n",
    "\n",
    "    # Append the cleaned review to the corpus list\n",
    "    corpus.append(review)\n",
    "\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "# max_features=100 limits the vocabulary to the top 100 most frequent terms\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned text corpus\n",
    "X = tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Matrix:\n",
      "\n",
      "       aa  aah  aaniye  aaooooright  aathi   ab  abbey  abdomen  abeg  abel  \\\n",
      "0     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "1     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "2     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "3     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "4     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "...   ...  ...     ...          ...    ...  ...    ...      ...   ...   ...   \n",
      "5569  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "5570  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "5571  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "5572  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "5573  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
      "\n",
      "      ...  zed  zero   zf  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  \n",
      "0     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "1     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "2     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "3     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "4     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "...   ...  ...   ...  ...    ...     ...  ...        ...   ...   ...    ...  \n",
      "5569  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "5570  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "5571  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "5572  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "5573  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
      "\n",
      "[5574 rows x 7098 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame for better readability\n",
    "df_tfidf = pd.DataFrame(X, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Display the result\n",
    "print(\"TF-IDF Feature Matrix:\\n\")\n",
    "print(df_tfidf.round(3))  # Round for better viewing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-IDF using N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('free entry', 31),\n",
       " ('claim call', 16),\n",
       " ('call claim', 3),\n",
       " ('free call', 30),\n",
       " ('chance win', 15)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the TF-IDF Vectorizer for bigrams\n",
    "# max_features=100 limits the number of top bigrams based on frequency\n",
    "tfidf = TfidfVectorizer(ngram_range=(2, 2), max_features=100)\n",
    "\n",
    "# Fit and transform the corpus into a TF-IDF-weighted matrix of bigrams\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "\n",
    "list(tfidf.vocabulary_.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (Bigrams):\n",
      "\n",
      "      account statement  attempt contact  await collection  call claim  \\\n",
      "0                   0.0              0.0               0.0         0.0   \n",
      "1                   0.0              0.0               0.0         0.0   \n",
      "2                   0.0              0.0               0.0         0.0   \n",
      "3                   0.0              0.0               0.0         0.0   \n",
      "4                   0.0              0.0               0.0         0.0   \n",
      "...                 ...              ...               ...         ...   \n",
      "5569                0.0              0.0               0.0         0.0   \n",
      "5570                0.0              0.0               0.0         0.0   \n",
      "5571                0.0              0.0               0.0         0.0   \n",
      "5572                0.0              0.0               0.0         0.0   \n",
      "5573                0.0              0.0               0.0         0.0   \n",
      "\n",
      "      call customer  call identifier  call land  call landline  call later  \\\n",
      "0               0.0              0.0        0.0            0.0         0.0   \n",
      "1               0.0              0.0        0.0            0.0         0.0   \n",
      "2               0.0              0.0        0.0            0.0         0.0   \n",
      "3               0.0              0.0        0.0            0.0         0.0   \n",
      "4               0.0              0.0        0.0            0.0         0.0   \n",
      "...             ...              ...        ...            ...         ...   \n",
      "5569            0.0              0.0        0.0            0.0         0.0   \n",
      "5570            0.0              0.0        0.0            0.0         0.0   \n",
      "5571            0.0              0.0        0.0            0.0         0.0   \n",
      "5572            0.0              0.0        0.0            0.0         0.0   \n",
      "5573            0.0              0.0        0.0            0.0         0.0   \n",
      "\n",
      "      call mobileupd  ...  ur awarded  ur cash  ur friend  ur mob  \\\n",
      "0                0.0  ...         0.0      0.0        0.0     0.0   \n",
      "1                0.0  ...         0.0      0.0        0.0     0.0   \n",
      "2                0.0  ...         0.0      0.0        0.0     0.0   \n",
      "3                0.0  ...         0.0      0.0        0.0     0.0   \n",
      "4                0.0  ...         0.0      0.0        0.0     0.0   \n",
      "...              ...  ...         ...      ...        ...     ...   \n",
      "5569             0.0  ...         0.0      0.0        0.0     0.0   \n",
      "5570             0.0  ...         0.0      0.0        0.0     0.0   \n",
      "5571             0.0  ...         0.0      0.0        0.0     0.0   \n",
      "5572             0.0  ...         0.0      0.0        0.0     0.0   \n",
      "5573             0.0  ...         0.0      0.0        0.0     0.0   \n",
      "\n",
      "      urgent mobile  valid hr  want come  want go  wat time  week txt  \n",
      "0               0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "1               0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "2               0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "3               0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "4               0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "...             ...       ...        ...      ...       ...       ...  \n",
      "5569            0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "5570            0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "5571            0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "5572            0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "5573            0.0       0.0        0.0      0.0       0.0       0.0  \n",
      "\n",
      "[5574 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert to a DataFrame for clearer display\n",
    "df_tfidf_bigrams = pd.DataFrame(X, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF matrix (rounded for readability)\n",
    "print(\"TF-IDF Matrix (Bigrams):\\n\")\n",
    "print(df_tfidf_bigrams.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
