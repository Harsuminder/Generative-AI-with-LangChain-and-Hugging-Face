{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔡 Word Embeddings: Capturing Semantic Meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Word Embeddings?\n",
    "\n",
    "**Word embeddings** are vector representations of words that capture their **meaning and relationships** based on context.  \n",
    "Each word is mapped to a **dense, real-valued vector** in a continuous vector space.\n",
    "\n",
    "Unlike Bag of Words or TF-IDF, which treat words as independent and sparse, embeddings place **similar words closer together** in the vector space.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Example\n",
    "\n",
    "| Word       | Embedding (3D example)      |\n",
    "|------------|-----------------------------|\n",
    "| king       | [0.21, 0.87, 0.34]           |\n",
    "| queen      | [0.20, 0.85, 0.36]           |\n",
    "| apple      | [-0.45, 0.12, 0.88]          |\n",
    "| banana     | [-0.43, 0.15, 0.85]          |\n",
    "\n",
    "👑 Words like `\"king\"` and `\"queen\"` have **similar vectors**, as they appear in similar contexts.\n",
    "\n",
    "🍎 Words like `\"apple\"` and `\"banana\"` also cluster together in space — indicating **semantic similarity**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Types of Word Embeddings\n",
    "\n",
    "Word embeddings can be broadly classified into **two main categories** based on how they are generated:\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Frequency-Based Embeddings\n",
    "\n",
    "These are traditional vectorization techniques based on **word counts** or **co-occurrence frequencies**.\n",
    "\n",
    "Examples include:\n",
    "- **One-Hot Encoding (OHE)**: Represents each word as a binary vector — high dimensional and sparse.\n",
    "- **Bag of Words (BoW)**: Uses word frequency per document, ignoring order and context.\n",
    "- **TF-IDF**: Weights words by importance using frequency and rarity across documents.\n",
    "\n",
    "> 🔹 These methods are simple, interpretable, and easy to implement, but they **lack semantic understanding** and context awareness.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Prediction-Based Embeddings (Deep Learning)\n",
    "\n",
    "These are **learned from data** using neural networks that understand context. They aim to predict surrounding words or the current word in a sentence.\n",
    "\n",
    "Examples include:\n",
    "- **CBOW (Continuous Bag of Words)**: Predicts the target word using surrounding context words.\n",
    "- **Skip-Gram**: Predicts surrounding context words from a single input word.\n",
    "\n",
    "> 🔹 These embeddings capture **semantic meaning**, relationships, and analogies (e.g., `\"king\" - \"man\" + \"woman\" ≈ \"queen\"`), and are widely used in modern NLP models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "                   ┌────────────────────────────┐\n",
    "                   │     Word Embeddings        │\n",
    "                   └────────────┬───────────────┘\n",
    "                                │\n",
    "                ┌───────────────┴───────────────┐\n",
    "                │                               │\n",
    " ┌────────────────────────┐       ┌──────────────────────────────┐\n",
    " │ 1. Frequency-Based     │       │ 2. Prediction-Based (DL)     │\n",
    " └────────────┬───────────┘       └──────────────┬───────────────┘\n",
    "              │                                  │\n",
    "  ┌───────────┴────────────┐        ┌────────────┴────────────┐\n",
    "  │ One-Hot Encoding (OHE) │        │     CBOW (Word2Vec)     │\n",
    "  │ Bag of Words (BoW)     │        │   Skip-Gram (Word2Vec)  │\n",
    "  │ TF-IDF                 │        │                         │\n",
    "  └────────────────────────┘        └─────────────────────────┘\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
