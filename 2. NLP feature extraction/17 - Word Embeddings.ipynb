{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¡ Word Embeddings: Capturing Semantic Meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Word Embeddings?\n",
    "\n",
    "**Word embeddings** are vector representations of words that capture their **meaning and relationships** based on context.  \n",
    "Each word is mapped to a **dense, real-valued vector** in a continuous vector space.\n",
    "\n",
    "Unlike Bag of Words or TF-IDF, which treat words as independent and sparse, embeddings place **similar words closer together** in the vector space.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Example\n",
    "\n",
    "| Word       | Embedding (3D example)      |\n",
    "|------------|-----------------------------|\n",
    "| king       | [0.21, 0.87, 0.34]           |\n",
    "| queen      | [0.20, 0.85, 0.36]           |\n",
    "| apple      | [-0.45, 0.12, 0.88]          |\n",
    "| banana     | [-0.43, 0.15, 0.85]          |\n",
    "\n",
    "ğŸ‘‘ Words like `\"king\"` and `\"queen\"` have **similar vectors**, as they appear in similar contexts.\n",
    "\n",
    "ğŸ Words like `\"apple\"` and `\"banana\"` also cluster together in space â€” indicating **semantic similarity**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§© Types of Word Embeddings\n",
    "\n",
    "Word embeddings can be broadly classified into **two main categories** based on how they are generated:\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Frequency-Based Embeddings\n",
    "\n",
    "These are traditional vectorization techniques based on **word counts** or **co-occurrence frequencies**.\n",
    "\n",
    "Examples include:\n",
    "- **One-Hot Encoding (OHE)**: Represents each word as a binary vector â€” high dimensional and sparse.\n",
    "- **Bag of Words (BoW)**: Uses word frequency per document, ignoring order and context.\n",
    "- **TF-IDF**: Weights words by importance using frequency and rarity across documents.\n",
    "\n",
    "> ğŸ”¹ These methods are simple, interpretable, and easy to implement, but they **lack semantic understanding** and context awareness.\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Prediction-Based Embeddings (Deep Learning)\n",
    "\n",
    "These are **learned from data** using neural networks that understand context. They aim to predict surrounding words or the current word in a sentence.\n",
    "\n",
    "Examples include:\n",
    "- **CBOW (Continuous Bag of Words)**: Predicts the target word using surrounding context words.\n",
    "- **Skip-Gram**: Predicts surrounding context words from a single input word.\n",
    "\n",
    "> ğŸ”¹ These embeddings capture **semantic meaning**, relationships, and analogies (e.g., `\"king\" - \"man\" + \"woman\" â‰ˆ \"queen\"`), and are widely used in modern NLP models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                   â”‚     Word Embeddings        â”‚\n",
    "                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                â”‚\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚                               â”‚\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " â”‚ 1. Frequency-Based     â”‚       â”‚ 2. Prediction-Based (DL)     â”‚\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚                                  â”‚\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ One-Hot Encoding (OHE) â”‚        â”‚     CBOW (Word2Vec)     â”‚\n",
    "  â”‚ Bag of Words (BoW)     â”‚        â”‚   Skip-Gram (Word2Vec)  â”‚\n",
    "  â”‚ TF-IDF                 â”‚        â”‚                         â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
