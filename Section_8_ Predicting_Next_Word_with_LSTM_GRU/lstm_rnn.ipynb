{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ace27b",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511eac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]    \n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef20404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Dataset\n",
    "data= gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "## Save dataset to a file\n",
    "with open('hamlet.txt', 'w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d82baa",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2063f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c3e454d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2712"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Dataset\n",
    "with open('hamlet.txt','r')as file:\n",
    "    text= file.read(70000).lower() # Limiting data read to 70,000 characters due to memory read issue\n",
    "\n",
    "\n",
    "## Tokenize the text\n",
    "tokenizer= Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words= len(tokenizer.word_index)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1dba3a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 611],\n",
       " [1, 611, 4],\n",
       " [1, 611, 4, 52],\n",
       " [1, 611, 4, 52, 38],\n",
       " [1, 611, 4, 52, 38, 980],\n",
       " [1, 611, 4, 52, 38, 980, 981],\n",
       " [1, 611, 4, 52, 38, 980, 981, 982],\n",
       " [612, 983],\n",
       " [612, 983, 984],\n",
       " [612, 983, 984, 985],\n",
       " [70, 219],\n",
       " [70, 219, 2],\n",
       " [70, 219, 2, 613],\n",
       " [70, 219, 2, 613, 220],\n",
       " [70, 219, 2, 613, 220, 986],\n",
       " [219, 614],\n",
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create input sequences\n",
    "input_sequence=[]\n",
    "for line in text.split('\\n'):\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence=token_list[:i+1]\n",
    "        input_sequence.append(n_gram_sequence)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13e3bc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pad Sequences\n",
    "max_sequence_len= max([len(x) for x in input_sequence])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "932d7e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  611],\n",
       "       [   0,    0,    0, ...,    1,  611,    4],\n",
       "       [   0,    0,    0, ...,  611,    4,   52],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    0,   39,  387],\n",
       "       [   0,    0,    0, ...,   39,  387,  673],\n",
       "       [   0,    0,    0, ...,  387,  673, 2711]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence= np.array(pad_sequences(input_sequence, maxlen=max_sequence_len, padding='pre'))\n",
    "#input_sequence=input_sequence[:35000]\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85107403",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create predictors and labels\n",
    "import tensorflow as tf\n",
    "\n",
    "x,y= input_sequence[:, :-1], input_sequence[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f01e8b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   1],\n",
       "       [  0,   0,   0, ...,   0,   1, 611],\n",
       "       [  0,   0,   0, ...,   1, 611,   4],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   0,   0,  39],\n",
       "       [  0,   0,   0, ...,   0,  39, 387],\n",
       "       [  0,   0,   0, ...,  39, 387, 673]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0819eb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 611,    4,   52, ...,  387,  673, 2711])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b9fac5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0fab6ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dda55d",
   "metadata": {},
   "source": [
    "## Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09192763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 13, 100)           271200    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2712)              273912    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 796112 (3.04 MB)\n",
      "Trainable params: 796112 (3.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM, Dense, Dropout\n",
    "\n",
    "## Define model\n",
    "model= Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "## Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac76d17",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d4eb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "276/276 [==============================] - 6s 21ms/step - loss: 3.6367 - accuracy: 0.2313 - val_loss: 9.3513 - val_accuracy: 0.0498\n",
      "Epoch 2/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 3.5775 - accuracy: 0.2362 - val_loss: 9.4267 - val_accuracy: 0.0534\n",
      "Epoch 3/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 3.5236 - accuracy: 0.2481 - val_loss: 9.5278 - val_accuracy: 0.0530\n",
      "Epoch 4/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 3.4674 - accuracy: 0.2528 - val_loss: 9.5972 - val_accuracy: 0.0507\n",
      "Epoch 5/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 3.4129 - accuracy: 0.2655 - val_loss: 9.7210 - val_accuracy: 0.0539\n",
      "Epoch 6/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 3.3663 - accuracy: 0.2676 - val_loss: 9.7843 - val_accuracy: 0.0489\n",
      "Epoch 7/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 3.3123 - accuracy: 0.2832 - val_loss: 9.8626 - val_accuracy: 0.0516\n",
      "Epoch 8/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 3.2679 - accuracy: 0.2908 - val_loss: 9.9605 - val_accuracy: 0.0498\n",
      "Epoch 9/100\n",
      "276/276 [==============================] - 6s 24ms/step - loss: 3.2186 - accuracy: 0.3024 - val_loss: 10.0221 - val_accuracy: 0.0539\n",
      "Epoch 10/100\n",
      "276/276 [==============================] - 8s 28ms/step - loss: 3.1769 - accuracy: 0.3101 - val_loss: 10.0999 - val_accuracy: 0.0462\n",
      "Epoch 11/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 3.1335 - accuracy: 0.3169 - val_loss: 10.1529 - val_accuracy: 0.0525\n",
      "Epoch 12/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 3.0833 - accuracy: 0.3307 - val_loss: 10.2600 - val_accuracy: 0.0494\n",
      "Epoch 13/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 3.0521 - accuracy: 0.3345 - val_loss: 10.3325 - val_accuracy: 0.0516\n",
      "Epoch 14/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 3.0145 - accuracy: 0.3368 - val_loss: 10.4167 - val_accuracy: 0.0476\n",
      "Epoch 15/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.9622 - accuracy: 0.3491 - val_loss: 10.4764 - val_accuracy: 0.0534\n",
      "Epoch 16/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.9232 - accuracy: 0.3559 - val_loss: 10.5558 - val_accuracy: 0.0525\n",
      "Epoch 17/100\n",
      "276/276 [==============================] - 6s 24ms/step - loss: 2.8873 - accuracy: 0.3645 - val_loss: 10.6495 - val_accuracy: 0.0462\n",
      "Epoch 18/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.8470 - accuracy: 0.3711 - val_loss: 10.6825 - val_accuracy: 0.0466\n",
      "Epoch 19/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.8090 - accuracy: 0.3795 - val_loss: 10.8160 - val_accuracy: 0.0462\n",
      "Epoch 20/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.7750 - accuracy: 0.3859 - val_loss: 10.8857 - val_accuracy: 0.0485\n",
      "Epoch 21/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.7361 - accuracy: 0.3928 - val_loss: 10.9333 - val_accuracy: 0.0494\n",
      "Epoch 22/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.7066 - accuracy: 0.4003 - val_loss: 10.9779 - val_accuracy: 0.0471\n",
      "Epoch 23/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.6710 - accuracy: 0.4104 - val_loss: 11.0604 - val_accuracy: 0.0480\n",
      "Epoch 24/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 2.6352 - accuracy: 0.4103 - val_loss: 11.1548 - val_accuracy: 0.0448\n",
      "Epoch 25/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.6035 - accuracy: 0.4206 - val_loss: 11.1934 - val_accuracy: 0.0430\n",
      "Epoch 26/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.5730 - accuracy: 0.4268 - val_loss: 11.2753 - val_accuracy: 0.0480\n",
      "Epoch 27/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.5356 - accuracy: 0.4372 - val_loss: 11.3319 - val_accuracy: 0.0480\n",
      "Epoch 28/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.5161 - accuracy: 0.4435 - val_loss: 11.3550 - val_accuracy: 0.0466\n",
      "Epoch 29/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.4839 - accuracy: 0.4483 - val_loss: 11.4533 - val_accuracy: 0.0453\n",
      "Epoch 30/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.4528 - accuracy: 0.4582 - val_loss: 11.5194 - val_accuracy: 0.0462\n",
      "Epoch 31/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.4171 - accuracy: 0.4616 - val_loss: 11.5569 - val_accuracy: 0.0466\n",
      "Epoch 32/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.3956 - accuracy: 0.4688 - val_loss: 11.6231 - val_accuracy: 0.0489\n",
      "Epoch 33/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.3629 - accuracy: 0.4679 - val_loss: 11.6737 - val_accuracy: 0.0448\n",
      "Epoch 34/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.3396 - accuracy: 0.4759 - val_loss: 11.8003 - val_accuracy: 0.0471\n",
      "Epoch 35/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.3157 - accuracy: 0.4820 - val_loss: 11.7997 - val_accuracy: 0.0462\n",
      "Epoch 36/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.2789 - accuracy: 0.4888 - val_loss: 11.8824 - val_accuracy: 0.0457\n",
      "Epoch 37/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 2.2632 - accuracy: 0.4952 - val_loss: 11.9257 - val_accuracy: 0.0444\n",
      "Epoch 38/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 2.2354 - accuracy: 0.5035 - val_loss: 11.9858 - val_accuracy: 0.0426\n",
      "Epoch 39/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.2051 - accuracy: 0.5072 - val_loss: 12.0408 - val_accuracy: 0.0485\n",
      "Epoch 40/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.1788 - accuracy: 0.5126 - val_loss: 12.1370 - val_accuracy: 0.0471\n",
      "Epoch 41/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.1560 - accuracy: 0.5089 - val_loss: 12.1528 - val_accuracy: 0.0421\n",
      "Epoch 42/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.1372 - accuracy: 0.5193 - val_loss: 12.1870 - val_accuracy: 0.0466\n",
      "Epoch 43/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 2.1127 - accuracy: 0.5214 - val_loss: 12.2692 - val_accuracy: 0.0453\n",
      "Epoch 44/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.0870 - accuracy: 0.5292 - val_loss: 12.3066 - val_accuracy: 0.0476\n",
      "Epoch 45/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.0654 - accuracy: 0.5366 - val_loss: 12.3768 - val_accuracy: 0.0439\n",
      "Epoch 46/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 2.0511 - accuracy: 0.5384 - val_loss: 12.4147 - val_accuracy: 0.0457\n",
      "Epoch 47/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 2.0242 - accuracy: 0.5435 - val_loss: 12.4300 - val_accuracy: 0.0457\n",
      "Epoch 48/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 2.0067 - accuracy: 0.5433 - val_loss: 12.5038 - val_accuracy: 0.0453\n",
      "Epoch 49/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.9730 - accuracy: 0.5543 - val_loss: 12.5668 - val_accuracy: 0.0439\n",
      "Epoch 50/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.9448 - accuracy: 0.5615 - val_loss: 12.5966 - val_accuracy: 0.0462\n",
      "Epoch 51/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.9404 - accuracy: 0.5603 - val_loss: 12.6605 - val_accuracy: 0.0457\n",
      "Epoch 52/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.9119 - accuracy: 0.5698 - val_loss: 12.7495 - val_accuracy: 0.0435\n",
      "Epoch 53/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.8879 - accuracy: 0.5699 - val_loss: 12.8033 - val_accuracy: 0.0426\n",
      "Epoch 54/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.8677 - accuracy: 0.5780 - val_loss: 12.8070 - val_accuracy: 0.0435\n",
      "Epoch 55/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.8523 - accuracy: 0.5804 - val_loss: 12.8642 - val_accuracy: 0.0435\n",
      "Epoch 56/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.8356 - accuracy: 0.5809 - val_loss: 12.9066 - val_accuracy: 0.0435\n",
      "Epoch 57/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.8095 - accuracy: 0.5856 - val_loss: 13.0096 - val_accuracy: 0.0426\n",
      "Epoch 58/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.7936 - accuracy: 0.5948 - val_loss: 13.0204 - val_accuracy: 0.0439\n",
      "Epoch 59/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.7739 - accuracy: 0.6004 - val_loss: 13.0854 - val_accuracy: 0.0421\n",
      "Epoch 60/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.7482 - accuracy: 0.6016 - val_loss: 13.1209 - val_accuracy: 0.0448\n",
      "Epoch 61/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.7394 - accuracy: 0.6042 - val_loss: 13.1691 - val_accuracy: 0.0439\n",
      "Epoch 62/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.7224 - accuracy: 0.6095 - val_loss: 13.2301 - val_accuracy: 0.0457\n",
      "Epoch 63/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.7060 - accuracy: 0.6112 - val_loss: 13.2090 - val_accuracy: 0.0453\n",
      "Epoch 64/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.6781 - accuracy: 0.6181 - val_loss: 13.2727 - val_accuracy: 0.0448\n",
      "Epoch 65/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.6684 - accuracy: 0.6153 - val_loss: 13.3303 - val_accuracy: 0.0444\n",
      "Epoch 66/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.6422 - accuracy: 0.6261 - val_loss: 13.3485 - val_accuracy: 0.0417\n",
      "Epoch 67/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.6312 - accuracy: 0.6287 - val_loss: 13.4122 - val_accuracy: 0.0453\n",
      "Epoch 68/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.6087 - accuracy: 0.6345 - val_loss: 13.4218 - val_accuracy: 0.0453\n",
      "Epoch 69/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.5988 - accuracy: 0.6360 - val_loss: 13.5053 - val_accuracy: 0.0426\n",
      "Epoch 70/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.5881 - accuracy: 0.6413 - val_loss: 13.5577 - val_accuracy: 0.0426\n",
      "Epoch 71/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.5733 - accuracy: 0.6370 - val_loss: 13.5723 - val_accuracy: 0.0430\n",
      "Epoch 72/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.5487 - accuracy: 0.6492 - val_loss: 13.6553 - val_accuracy: 0.0453\n",
      "Epoch 73/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.5324 - accuracy: 0.6513 - val_loss: 13.6572 - val_accuracy: 0.0457\n",
      "Epoch 74/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 1.5232 - accuracy: 0.6478 - val_loss: 13.7158 - val_accuracy: 0.0453\n",
      "Epoch 75/100\n",
      "276/276 [==============================] - 7s 27ms/step - loss: 1.5019 - accuracy: 0.6579 - val_loss: 13.7400 - val_accuracy: 0.0430\n",
      "Epoch 76/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.4901 - accuracy: 0.6620 - val_loss: 13.7813 - val_accuracy: 0.0426\n",
      "Epoch 77/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.4771 - accuracy: 0.6631 - val_loss: 13.8622 - val_accuracy: 0.0462\n",
      "Epoch 78/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.4594 - accuracy: 0.6657 - val_loss: 13.8972 - val_accuracy: 0.0430\n",
      "Epoch 79/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.4426 - accuracy: 0.6686 - val_loss: 13.8921 - val_accuracy: 0.0453\n",
      "Epoch 80/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.4374 - accuracy: 0.6716 - val_loss: 13.9523 - val_accuracy: 0.0453\n",
      "Epoch 81/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.4129 - accuracy: 0.6783 - val_loss: 13.9688 - val_accuracy: 0.0453\n",
      "Epoch 82/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 1.4072 - accuracy: 0.6763 - val_loss: 14.0834 - val_accuracy: 0.0439\n",
      "Epoch 83/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 1.3971 - accuracy: 0.6808 - val_loss: 14.0675 - val_accuracy: 0.0448\n",
      "Epoch 84/100\n",
      "276/276 [==============================] - 7s 26ms/step - loss: 1.3728 - accuracy: 0.6821 - val_loss: 14.1255 - val_accuracy: 0.0462\n",
      "Epoch 85/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.3649 - accuracy: 0.6863 - val_loss: 14.2186 - val_accuracy: 0.0494\n",
      "Epoch 86/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.3506 - accuracy: 0.6892 - val_loss: 14.2027 - val_accuracy: 0.0457\n",
      "Epoch 87/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.3361 - accuracy: 0.6931 - val_loss: 14.1963 - val_accuracy: 0.0444\n",
      "Epoch 88/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.3196 - accuracy: 0.6972 - val_loss: 14.2450 - val_accuracy: 0.0462\n",
      "Epoch 89/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.3143 - accuracy: 0.6951 - val_loss: 14.3180 - val_accuracy: 0.0448\n",
      "Epoch 90/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.3107 - accuracy: 0.6976 - val_loss: 14.3274 - val_accuracy: 0.0426\n",
      "Epoch 91/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.2946 - accuracy: 0.7004 - val_loss: 14.3872 - val_accuracy: 0.0412\n",
      "Epoch 92/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.2742 - accuracy: 0.7036 - val_loss: 14.4004 - val_accuracy: 0.0421\n",
      "Epoch 93/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.2678 - accuracy: 0.7079 - val_loss: 14.4598 - val_accuracy: 0.0439\n",
      "Epoch 94/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.2519 - accuracy: 0.7143 - val_loss: 14.4734 - val_accuracy: 0.0421\n",
      "Epoch 95/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.2396 - accuracy: 0.7108 - val_loss: 14.5213 - val_accuracy: 0.0412\n",
      "Epoch 96/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.2287 - accuracy: 0.7194 - val_loss: 14.5513 - val_accuracy: 0.0435\n",
      "Epoch 97/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.2217 - accuracy: 0.7181 - val_loss: 14.5770 - val_accuracy: 0.0421\n",
      "Epoch 98/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.2101 - accuracy: 0.7204 - val_loss: 14.5443 - val_accuracy: 0.0448\n",
      "Epoch 99/100\n",
      "276/276 [==============================] - 7s 25ms/step - loss: 1.1921 - accuracy: 0.7287 - val_loss: 14.6709 - val_accuracy: 0.0430\n",
      "Epoch 100/100\n",
      "276/276 [==============================] - 7s 24ms/step - loss: 1.1872 - accuracy: 0.7255 - val_loss: 14.7124 - val_accuracy: 0.0426\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=100, validation_data=(x_test,y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d28f18",
   "metadata": {},
   "source": [
    "## Prediction Function\n",
    "\n",
    "This function predicts the next word given an input text using a trained LSTM model.\n",
    "\n",
    "### 🔍 How It Works:\n",
    "1. **Tokenization**: Converts the input text into a sequence of integers.\n",
    "2. **Trimming**: Ensures the sequence fits within the `max_sequence_len - 1` constraint by retaining only the most recent tokens if needed.\n",
    "3. **Padding**: Pads the sequence from the left to match the input length expected by the model.\n",
    "4. **Prediction**: Uses the model to predict the next word’s index.\n",
    "5. **Decoding**: Finds the corresponding word from the tokenizer’s word index.\n",
    "\n",
    "Returns the predicted word or `None` if not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c48c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text, tokenizer, model, max_sequence_len):\n",
    "    token_list= tokenizer.texts_to_sequences([input_text])[0]\n",
    "    if len(token_list)>=max_sequence_len:\n",
    "        token_list=token_list[-(max_sequence_len-1):]\n",
    "    token_list= pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted= model.predict(token_list,verbose=0)\n",
    "    predicted_word_index= np.argmax(predicted, axis=1)  # We extract the index of the highest probability word\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index== predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d93fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text  You are a\n",
      " Predicted word: catch\n"
     ]
    }
   ],
   "source": [
    "input_text= \" You are a\"\n",
    "max_sequence_len= model.input_shape[1]+1\n",
    "print(f\"Input text {input_text}\")\n",
    "next_word= predict_next_word(input_text, tokenizer, model, max_sequence_len )\n",
    "print(f\" Predicted word: {next_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e076e",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63274d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsu\\.conda\\envs\\tensorflow\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"next_word_lstm.h5\")\n",
    "\n",
    "# save Tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) #protocol=pickle.HIGHEST_PROTOCOL ensures the most efficient and latest available pickle format is used for compatibility and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94838d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
