# Generative AI with LangChain and Hugging Face

## Overview
This repository contains implementations and examples on Generative AI using **LangChain** and **Hugging Face**. It covers fundamental NLP preprocessing techniques and will expand to include LangChain applications and transformer-based models.

## Repository Structure
```
Generative-AI-with-LangChain-and-Hugging-Face/
â”‚-- 2. NLP feature extraction/
â”‚   â”œâ”€â”€ 1-Tokenization Example.ipynb
â”‚   â”œâ”€â”€ 2-Stemming.ipynb
â”‚   â”œâ”€â”€ 3-Lemmatization.ipynb
â”‚   â”œâ”€â”€ 4-Stop Words.ipynb
â”‚   â”œâ”€â”€ 5-Parts of Speech Tagging.ipynb
â”‚   â”œâ”€â”€ 6-Named Entity Recognition.ipynb
â”‚   â”œâ”€â”€ 7-One Hot Encoding-P1.ipynb
â”‚   â”œâ”€â”€ 8-One Hot Encoding-P2.ipynb
â”‚   â”œâ”€â”€ 9-Bag of Words-P1.ipynb
â”‚   â”œâ”€â”€ 10 - Bag of Words-P2.ipynb
â”‚   â”œâ”€â”€ 11 - BOW Implementation using NLTK.ipynb
â”‚   â”œâ”€â”€ 12 - N-Grams- P1.ipynb
â”‚   â”œâ”€â”€ 13 - N-gram Implementation using NLTK.ipynb
â”‚-- Datasets/
â”‚   â””â”€â”€ spamclassification.csv
```

## Contents
### NLP Preprocessing
#### 1ï¸âƒ£ Tokenization
- **File:** `1-Tokenization Example.ipynb`
- **Description:** Demonstrates tokenization techniques using libraries like **NLTK, SpaCy, and Hugging Face Tokenizers**.
- **Key Topics:**
  - Word Tokenization
  - Sentence Tokenization
  - Subword Tokenization

#### 2ï¸âƒ£ Stemming
- **File:** `2-Stemming.ipynb`
- **Description:** Shows how stemming reduces words to their root form using **NLTKâ€™s PorterStemmer and SnowballStemmer**.
- **Key Topics:**
  - Porter Stemmer
  - Snowball Stemmer
  - Differences between Stemming and Lemmatization

#### 3ï¸âƒ£ Lemmatization
- **File:** `3-Lemmatization.ipynb`
- **Description:** Explores **lemmatization**, which reduces words to their base form while maintaining meaning.
- **Key Topics:**
  - Lemmatization with WordNetLemmatizer
  - Comparison with Stemming
  - Why lemmatization preserves meaning better

#### 4ï¸âƒ£ Stop Words
- **File:** `4-Stop Words.ipynb`
- **Description:** Removes common words that do not add meaning using NLTK stopword corpus.
- **Key Topics:**
  - Importance of filtering
  - NLTK stopword list
  - Efficiency in modeling
  - Integration with pipelines

#### 5ï¸âƒ£ POS Tagging
- **File:** `5-Parts of Speech Tagging.ipynb`
- **Description:** Assigns grammatical tags to words for structural analysis.
- **Key Topics:**
  - POS tag types
  - NLTK pos_tag usage
  - Grammar-based preprocessing
  - Use in NER & lemmatization

#### 6ï¸âƒ£ Named Entity Recognition
- **File:** `6-Named Entity Recognition.ipynb`
- **Description:** Identifies entities like names, organizations, locations using NLTK's chunker.
- **Key Topics:**
  - NER categories
  - Chunk parsing
  - Named entity extraction
  - Use in info retrieval

#### 7ï¸âƒ£ One-Hot Encoding â€“ Basics
- **File:** `7-One Hot Encoding-P1.ipynb`
- **Description:** Introduces one-hot encoding for word representation with binary vectors.
- **Key Topics:**
  - Vocabulary indexing
  - Binary vectorization
  - Manual examples
  - Role in ML input

#### 8ï¸âƒ£ One-Hot Encoding â€“ Analysis
- **File:** `8-One Hot Encoding-P2.ipynb`
- **Description:** Discusses pros and cons of one-hot encoding for NLP tasks.
- **Key Topics:**
  - Sparsity issues
  - Lack of semantic context
  - High dimensionality
  - No OOV handling

#### 9ï¸âƒ£ Bag of Words â€“ Concept
- **File:** `9-Bag of Words-P1.ipynb`
- **Description:** Converts sentences to word frequency vectors using a fixed vocabulary.
- **Key Topics:**
  - Document-term matrix
  - Token counting
  - Stopword filtering
  - Vocabulary creation

#### ğŸ”Ÿ Bag of Words â€“ Evaluation
- **File:** `10 - Bag of Words-P2.ipynb`
- **Description:** Highlights the strengths and weaknesses of BoW for NLP.
- **Key Topics:**
  - No order/context awareness
  - Sparse matrix structure
  - Effective for classification
  - Cannot handle OOV

#### 1ï¸âƒ£1 BoW from Scratch
- **File:** `11 - BOW Implementation using NLTK.ipynb`
- **Description:** Builds a Bag of Words model using pure Python and NLTK, without scikit-learn.
- **Key Topics:**
  - Text preprocessing & cleaning
  - Tokenization and frequency counting
  - Vector space model basics
  - Foundational understanding of BoW

#### 1ï¸âƒ£2 N-Grams â€“ Intuition
- **File:** `12 - N-Grams- P1.ipynb`
- **Description:** Demonstrates how bigrams and trigrams capture meaning better than BoW.
- **Key Topics:**
  - N-gram vs BoW comparison
  - Use of `ngram_range`
  - Contextual meaning in sentiment
  - Example-driven explanation

#### 1ï¸âƒ£3 N-Grams with NLTK
- **File:** `13 - N-gram Implementation using NLTK.ipynb`
- **Description:** Uses NLTK to generate unigrams, bigrams, and trigrams from raw text.
- **Key Topics:**
  - `nltk.ngrams()` usage
  - Manual n-gram generation
  - Tokenization workflow
  - Real text examples

---

## ğŸ“ Medium Article Series

Read the companion blogs on Medium: ğŸ“˜ [@harsuminder](https://medium.com/@harsuminder)

---
## Installation & Setup
To run the notebooks locally, follow these steps:
```bash
# Clone the repository
git clone https://github.com/Harsuminder/Generative-AI-with-LangChain-and-Hugging-Face.git
cd Generative-AI-with-LangChain-and-Hugging-Face

# Run Jupyter Notebook
jupyter notebook
```

## Future Work
ğŸ”¹ **LangChain Experiments** (Coming Soon)  
ğŸ”¹ **Hugging Face Transformer Models** (Coming Soon)  
ğŸ”¹ **Fine-Tuning LLMs for Applications** (Coming Soon)  

## Author
**Harsuminder Kaur Gill**  
ğŸ”— [GitHub](https://github.com/Harsuminder) | ğŸ”— [LinkedIn](https://www.linkedin.com/in/harsuminder/) | ğŸ”— [Medium](https://medium.com/@harsuminder)

## Contributions
Feel free to fork this repo, open issues, or submit pull requests to improve it!
